{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-18T15:56:48.694719Z","iopub.execute_input":"2023-03-18T15:56:48.695337Z","iopub.status.idle":"2023-03-18T15:56:48.757164Z","shell.execute_reply.started":"2023-03-18T15:56:48.695210Z","shell.execute_reply":"2023-03-18T15:56:48.755813Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/emotion-dataset/validation.csv\n/kaggle/input/emotion-dataset/training.csv\n/kaggle/input/emotion-dataset/test.csv\n/kaggle/input/contractions/contractions.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"a classification label, with possible values including sadness (0), joy (1), love (2), anger (3), fear (4),surprise(5).","metadata":{}},{"cell_type":"code","source":"emo_la={0:'sadness',1:'joy',2:'love',3:'anger',4:'fear',5:'surprise'}","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:56:48.760297Z","iopub.execute_input":"2023-03-18T15:56:48.761210Z","iopub.status.idle":"2023-03-18T15:56:48.768176Z","shell.execute_reply.started":"2023-03-18T15:56:48.761155Z","shell.execute_reply":"2023-03-18T15:56:48.766694Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"emo_la[1]","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:56:48.770137Z","iopub.execute_input":"2023-03-18T15:56:48.770816Z","iopub.status.idle":"2023-03-18T15:56:48.786191Z","shell.execute_reply.started":"2023-03-18T15:56:48.770754Z","shell.execute_reply":"2023-03-18T15:56:48.784580Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'joy'"},"metadata":{}}]},{"cell_type":"code","source":"data=pd.read_csv('../input/emotion-dataset/training.csv')\ndata","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:56:48.792284Z","iopub.execute_input":"2023-03-18T15:56:48.793671Z","iopub.status.idle":"2023-03-18T15:56:48.896512Z","shell.execute_reply.started":"2023-03-18T15:56:48.793571Z","shell.execute_reply":"2023-03-18T15:56:48.894973Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                    text  label\n0                                i didnt feel humiliated      0\n1      i can go from feeling so hopeless to so damned...      0\n2       im grabbing a minute to post i feel greedy wrong      3\n3      i am ever feeling nostalgic about the fireplac...      2\n4                                   i am feeling grouchy      3\n...                                                  ...    ...\n15995  i just had a very brief time in the beanbag an...      0\n15996  i am now turning and i feel pathetic that i am...      0\n15997                     i feel strong and good overall      1\n15998  i feel like this was such a rude comment and i...      3\n15999  i know a lot but i feel so stupid because i ca...      0\n\n[16000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>i didnt feel humiliated</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i can go from feeling so hopeless to so damned...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>im grabbing a minute to post i feel greedy wrong</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i am ever feeling nostalgic about the fireplac...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i am feeling grouchy</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15995</th>\n      <td>i just had a very brief time in the beanbag an...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15996</th>\n      <td>i am now turning and i feel pathetic that i am...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15997</th>\n      <td>i feel strong and good overall</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15998</th>\n      <td>i feel like this was such a rude comment and i...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>15999</th>\n      <td>i know a lot but i feel so stupid because i ca...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>16000 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 10))\nsns.barplot(x=data['label'].value_counts().index,y=data['label'].value_counts().values)\npalette_color = sns.color_palette('bright')\n  \n\n  \n# displaying chart\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:56:48.899143Z","iopub.execute_input":"2023-03-18T15:56:48.900006Z","iopub.status.idle":"2023-03-18T15:56:50.489047Z","shell.execute_reply.started":"2023-03-18T15:56:48.899955Z","shell.execute_reply":"2023-03-18T15:56:50.487691Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 720x720 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"plt.pie(data['label'].value_counts(), labels=data['label'].apply(lambda x:emo_la[x]).value_counts().index, colors=palette_color, autopct='%.0f%%')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:56:50.491066Z","iopub.execute_input":"2023-03-18T15:56:50.491862Z","iopub.status.idle":"2023-03-18T15:56:50.650230Z","shell.execute_reply.started":"2023-03-18T15:56:50.491812Z","shell.execute_reply":"2023-03-18T15:56:50.648089Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"pd.options.display.max_colwidth = 1000\ndata[data['label']==5].loc[32]\n","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:56:50.655341Z","iopub.execute_input":"2023-03-18T15:56:50.656721Z","iopub.status.idle":"2023-03-18T15:56:50.687282Z","shell.execute_reply.started":"2023-03-18T15:56:50.656643Z","shell.execute_reply":"2023-03-18T15:56:50.685428Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"text     i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies\nlabel                                                                                                                   5\nName: 32, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"import nltk\nnltk.download('omw-1.4')\nfrom nltk.stem import PorterStemmer\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport gensim\n\nimport re\ndata_cl=data.copy()\ncontractions=pd.read_csv('../input/contractions/contractions.csv',index_col='Contraction')\ncontractions.index = contractions.index.str.lower()\ncontractions.Meaning = contractions.Meaning.str.lower()\ncontractions_dict = contractions.to_dict()['Meaning']\nk=stopwords.words('english')\n\nk=[i for i in k if i!='not']\n","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:56:50.690338Z","iopub.execute_input":"2023-03-18T15:56:50.691610Z","iopub.status.idle":"2023-03-18T15:56:52.458186Z","shell.execute_reply.started":"2023-03-18T15:56:50.691525Z","shell.execute_reply":"2023-03-18T15:56:52.456827Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"}]},{"cell_type":"code","source":"urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|(www\\.)[^ ]*)\"\nuserPattern       = '@[^\\s]+'\nhashtagPattern    = '#[^\\s]+'\nalphaPattern      = \"[^a-z0-9<>]\"\nsequencePattern   = r\"(.)\\1\\1+\"\nseqReplacePattern = r\"\\1\\1\"\n\n# Defining regex for emojis\nsmileemoji        = r\"[8:=;]['`\\-]?[)d]+\"\nsademoji          = r\"[8:=;]['`\\-]?\\(+\"\nneutralemoji      = r\"[8:=;]['`\\-]?[\\/|l*]\"\nlolemoji          = r\"[8:=;]['`\\-]?p+\"\nlemmatizer = WordNetLemmatizer()\ndef preprocess_apply(tweet):\n\n    tweet = tweet.lower()\n\n    # Replace all URls with '<url>'\n    tweet = re.sub(urlPattern,'<url>',tweet)\n    # Replace @USERNAME to '<user>'.\n    tweet = re.sub(userPattern,'<user>', tweet)\n    \n    # Replace 3 or more consecutive letters by 2 letter.\n    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n\n    # Replace all emojis.\n    tweet = re.sub(r'<3', '<heart>', tweet)\n    tweet = re.sub(smileemoji, '<smile>', tweet)\n    tweet = re.sub(sademoji, '<sadface>', tweet)\n    tweet = re.sub(neutralemoji, '<neutralface>', tweet)\n    tweet = re.sub(lolemoji, '<lolface>', tweet)\n\n    for contraction, replacement in contractions_dict.items():\n        tweet = tweet.replace(contraction, replacement)\n\n    # Remove non-alphanumeric and symbols\n    tweet = re.sub(alphaPattern, ' ', tweet)\n\n    # Adding space on either side of '/' to seperate words (After replacing URLS).\n    tweet = re.sub(r'/', ' / ', tweet)\n    tweet = nltk.word_tokenize(tweet)\n    tweet = [lemmatizer.lemmatize(sentence)  for sentence in tweet if sentence not in k]\n        #if sentence not in stopwords.words('english')\n    return tweet","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:56:52.459935Z","iopub.execute_input":"2023-03-18T15:56:52.460432Z","iopub.status.idle":"2023-03-18T15:56:52.477531Z","shell.execute_reply.started":"2023-03-18T15:56:52.460379Z","shell.execute_reply":"2023-03-18T15:56:52.475954Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def text_cleaning(x):\n    sentences=[]\n    stemmer = PorterStemmer()\n    lemmatizer = WordNetLemmatizer()\n    for i in range(1):\n        paragraph=x\n        text = re.sub(r'\\[[0-9]*\\]',' ',paragraph)\n        text =' '.join(word for word in text.split(' ') if not word.startswith('@'))\n        text = re.sub(r\"[^a-zA-Z0-9 ]\", \"\", text)\n        sentenc = nltk.word_tokenize(text)\n        #sentenc = sentenc.split()\n        #print(sentenc)\n        sentenc = [lemmatizer.lemmatize(sentence)  for sentence in sentenc if sentence not in stopwords.words('english')]\n        #if sentence not in stopwords.words('english')\n    return sentenc   ","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:56:52.479379Z","iopub.execute_input":"2023-03-18T15:56:52.480356Z","iopub.status.idle":"2023-03-18T15:56:52.496185Z","shell.execute_reply.started":"2023-03-18T15:56:52.480284Z","shell.execute_reply":"2023-03-18T15:56:52.494787Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data_cl['text']=data['text'].apply(lambda x:preprocess_apply(x))","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:56:52.497851Z","iopub.execute_input":"2023-03-18T15:56:52.499030Z","iopub.status.idle":"2023-03-18T15:56:59.837457Z","shell.execute_reply.started":"2023-03-18T15:56:52.498967Z","shell.execute_reply":"2023-03-18T15:56:59.836373Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"preprocess_apply(\"  not sad \")\n","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:56:59.839016Z","iopub.execute_input":"2023-03-18T15:56:59.839663Z","iopub.status.idle":"2023-03-18T15:56:59.846734Z","shell.execute_reply.started":"2023-03-18T15:56:59.839625Z","shell.execute_reply":"2023-03-18T15:56:59.845522Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"['not', 'sad']"},"metadata":{}}]},{"cell_type":"code","source":"# for i in range(len(data_cl['text'])):\n#     if any('servant' in z for z in X_test.loc[i]):\n#         print(i,'yes')\n#         break\n    \n# data_cl.loc[6,'text']","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:56:59.850018Z","iopub.execute_input":"2023-03-18T15:56:59.850403Z","iopub.status.idle":"2023-03-18T15:56:59.869384Z","shell.execute_reply.started":"2023-03-18T15:56:59.850370Z","shell.execute_reply":"2023-03-18T15:56:59.868176Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"data_cl['text']=data_cl['text'].map(lambda x:' '.join(x))\ndata_cl.loc[6,'text']","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:56:59.871186Z","iopub.execute_input":"2023-03-18T15:56:59.871523Z","iopub.status.idle":"2023-03-18T15:56:59.902562Z","shell.execute_reply.started":"2023-03-18T15:56:59.871494Z","shell.execute_reply":"2023-03-18T15:56:59.901328Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'ive taking milligram time recommended amount ive fallen asleep lot faster also feel like funny'"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ncv = TfidfVectorizer()\ncv=TfidfVectorizer(max_features=3000).fit(data_cl['text'])\nX = cv.fit_transform(data_cl['text']).toarray()\nX.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:42:16.824898Z","iopub.execute_input":"2023-03-17T15:42:16.825998Z","iopub.status.idle":"2023-03-17T15:42:17.689070Z","shell.execute_reply.started":"2023-03-17T15:42:16.825956Z","shell.execute_reply":"2023-03-17T15:42:17.687724Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(16000, 3000)"},"metadata":{}}]},{"cell_type":"code","source":"y=data_cl['label']","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:42:17.690976Z","iopub.execute_input":"2023-03-17T15:42:17.691448Z","iopub.status.idle":"2023-03-17T15:42:17.696538Z","shell.execute_reply.started":"2023-03-17T15:42:17.691402Z","shell.execute_reply":"2023-03-17T15:42:17.695364Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:42:17.698320Z","iopub.execute_input":"2023-03-17T15:42:17.699039Z","iopub.status.idle":"2023-03-17T15:42:17.976642Z","shell.execute_reply.started":"2023-03-17T15:42:17.698982Z","shell.execute_reply":"2023-03-17T15:42:17.975316Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix,accuracy_score,f1_score\n# Fitting Random Forest Classification to the Training set\nclassifier = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', random_state = 42)\nclassifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:42:17.978188Z","iopub.execute_input":"2023-03-17T15:42:17.978565Z","iopub.status.idle":"2023-03-17T15:42:34.560000Z","shell.execute_reply.started":"2023-03-17T15:42:17.978529Z","shell.execute_reply":"2023-03-17T15:42:34.559072Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"RandomForestClassifier(criterion='entropy', n_estimators=20, random_state=42)"},"metadata":{}}]},{"cell_type":"code","source":"X_test=pd.read_csv('../input/emotion-dataset/validation.csv')\ny_test=X_test['label']\nX_test=X_test.drop(columns=['label'])\nX_test['text']=X_test['text'].apply(lambda x:text_cleaning(x))\nX_test['text']=X_test['text'].map(lambda x:' '.join(x))\nX_test = cv.transform(X_test['text'])\nX_test","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:42:34.565467Z","iopub.execute_input":"2023-03-17T15:42:34.566314Z","iopub.status.idle":"2023-03-17T15:42:40.330060Z","shell.execute_reply.started":"2023-03-17T15:42:34.566268Z","shell.execute_reply":"2023-03-17T15:42:40.328841Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<2000x3000 sparse matrix of type '<class 'numpy.float64'>'\n\twith 15545 stored elements in Compressed Sparse Row format>"},"metadata":{}}]},{"cell_type":"code","source":"X_test","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:42:40.331487Z","iopub.execute_input":"2023-03-17T15:42:40.331843Z","iopub.status.idle":"2023-03-17T15:42:40.338807Z","shell.execute_reply.started":"2023-03-17T15:42:40.331811Z","shell.execute_reply":"2023-03-17T15:42:40.337463Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"<2000x3000 sparse matrix of type '<class 'numpy.float64'>'\n\twith 15545 stored elements in Compressed Sparse Row format>"},"metadata":{}}]},{"cell_type":"code","source":"\ny_pred = classifier.predict(X_test)\nprint('Testing accuracy %s' % accuracy_score(y_test, y_pred))\nprint('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:42:40.341008Z","iopub.execute_input":"2023-03-17T15:42:40.342570Z","iopub.status.idle":"2023-03-17T15:42:40.399033Z","shell.execute_reply.started":"2023-03-17T15:42:40.342512Z","shell.execute_reply":"2023-03-17T15:42:40.397742Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Testing accuracy 0.8765\nTesting F1 score: 0.8765916732070523\n","output_type":"stream"}]},{"cell_type":"code","source":"text='happy'\ntext=list(str(text).split(\".\"))\ntext=pd.Series(text)\nprint(text)\ntext=text.apply(lambda x:preprocess_apply(x))\nprint(text)\ntext=text.apply(lambda x:' '.join(x))\nprint(text)\ntext = cv.transform(text)\nemo_la[classifier.predict(text)[0]]\n[emo_la[s] for s in classifier.predict(text)]","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:42:40.400560Z","iopub.execute_input":"2023-03-17T15:42:40.401739Z","iopub.status.idle":"2023-03-17T15:42:40.427029Z","shell.execute_reply.started":"2023-03-17T15:42:40.401688Z","shell.execute_reply":"2023-03-17T15:42:40.425444Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"0    happy\ndtype: object\n0    [happy]\ndtype: object\n0    happy\ndtype: object\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"['joy']"},"metadata":{}}]},{"cell_type":"code","source":"import pickle\nfilename = 'finalized_model.pkl'\nwith open(filename,'wb') as f:\n    pickle.dump(classifier,f)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:42:40.428596Z","iopub.execute_input":"2023-03-17T15:42:40.429073Z","iopub.status.idle":"2023-03-17T15:42:40.459007Z","shell.execute_reply.started":"2023-03-17T15:42:40.429031Z","shell.execute_reply":"2023-03-17T15:42:40.457711Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import pickle\nfilename = 'finalized_model_tfidf.pkl'\nwith open(filename,'wb') as f:\n    pickle.dump(cv,f)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:42:40.461224Z","iopub.execute_input":"2023-03-17T15:42:40.461653Z","iopub.status.idle":"2023-03-17T15:42:40.482292Z","shell.execute_reply.started":"2023-03-17T15:42:40.461617Z","shell.execute_reply":"2023-03-17T15:42:40.481005Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n#tokenize and tag the card text\ncard_docs = [TaggedDocument(doc.split(' '), [i]) \n             for i, doc in enumerate(data_cl.text)]\n#display the tagged docs\n#card_docs","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:42:40.484433Z","iopub.execute_input":"2023-03-17T15:42:40.484990Z","iopub.status.idle":"2023-03-17T15:42:40.720067Z","shell.execute_reply.started":"2023-03-17T15:42:40.484937Z","shell.execute_reply":"2023-03-17T15:42:40.718717Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#instantiate model\nmodel = Doc2Vec(vector_size=24, window=2, min_count=1, workers=8, epochs = 100)\n#build vocab\nmodel.build_vocab(card_docs)\n#train model\nmodel.train(card_docs, total_examples=model.corpus_count\n            , epochs=model.epochs)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:42:40.723057Z","iopub.execute_input":"2023-03-17T15:42:40.725184Z","iopub.status.idle":"2023-03-17T15:45:42.140739Z","shell.execute_reply.started":"2023-03-17T15:42:40.725140Z","shell.execute_reply":"2023-03-17T15:45:42.138965Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#generate vectors\ncard2vec = [model.infer_vector((data_cl.text[i].split(' '))) \n            for i in range(0,len(data_cl.text))]\n#card2vec\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:45:42.142517Z","iopub.execute_input":"2023-03-17T15:45:42.143045Z","iopub.status.idle":"2023-03-17T15:46:44.302106Z","shell.execute_reply.started":"2023-03-17T15:45:42.142996Z","shell.execute_reply":"2023-03-17T15:46:44.301058Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"#Create a list of lists\ndtv= np.array(card2vec).tolist()\n#set list to dataframe column\ndata_cl['card2vec'] = dtv\ndata_cl.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:46:44.304017Z","iopub.execute_input":"2023-03-17T15:46:44.304961Z","iopub.status.idle":"2023-03-17T15:46:44.358152Z","shell.execute_reply.started":"2023-03-17T15:46:44.304815Z","shell.execute_reply":"2023-03-17T15:46:44.356961Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"                                                           text  label  \\\n0                                         didnt feel humiliated      0   \n1  go feeling hopeless damned hopeful around someone care awake      0   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      card2vec  \n0  [-0.6907301545143127, 0.3293410539627075, -0.13407957553863525, 0.4154246151447296, 0.45463377237319946, -0.16794486343860626, 0.17455363273620605, -0.2075994908809662, -0.5960902571678162, 0.29433995485305786, 0.14817246794700623, -0.002240437548607588, -0.25236013531684875, -0.01364750787615776, -0.48054972290992737, 0.6057925224304199, -0.5216194987297058, 0.20189762115478516, -0.2368055135011673, 0.16476386785507202, 0.4724339246749878, -0.07186953723430634, 1.3432865142822266, 0.21347837150096893]  \n1                  [-2.330378532409668, -0.6581130027770996, 0.5366047024726868, 2.1504430770874023, 0.6896774768829346, 1.182678461074829, -0.35320109128952026, -0.22903013229370117, 1.3991888761520386, 0.2835998237133026, 0.6544948816299438, 0.06605725735425949, 0.10888560116291046, -0.7360866665840149, -0.5467789173126221, 1.7088967561721802, -0.5841893553733826, -0.7892470955848694, 1.5171515941619873, 0.6034523248672485, 0.247245192527771, -0.2930087149143219, -1.3872815370559692, 0.6490201950073242]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>card2vec</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>didnt feel humiliated</td>\n      <td>0</td>\n      <td>[-0.6907301545143127, 0.3293410539627075, -0.13407957553863525, 0.4154246151447296, 0.45463377237319946, -0.16794486343860626, 0.17455363273620605, -0.2075994908809662, -0.5960902571678162, 0.29433995485305786, 0.14817246794700623, -0.002240437548607588, -0.25236013531684875, -0.01364750787615776, -0.48054972290992737, 0.6057925224304199, -0.5216194987297058, 0.20189762115478516, -0.2368055135011673, 0.16476386785507202, 0.4724339246749878, -0.07186953723430634, 1.3432865142822266, 0.21347837150096893]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>go feeling hopeless damned hopeful around someone care awake</td>\n      <td>0</td>\n      <td>[-2.330378532409668, -0.6581130027770996, 0.5366047024726868, 2.1504430770874023, 0.6896774768829346, 1.182678461074829, -0.35320109128952026, -0.22903013229370117, 1.3991888761520386, 0.2835998237133026, 0.6544948816299438, 0.06605725735425949, 0.10888560116291046, -0.7360866665840149, -0.5467789173126221, 1.7088967561721802, -0.5841893553733826, -0.7892470955848694, 1.5171515941619873, 0.6034523248672485, 0.247245192527771, -0.2930087149143219, -1.3872815370559692, 0.6490201950073242]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X=data_cl['card2vec']\ny=data_cl['label']","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:46:44.360071Z","iopub.execute_input":"2023-03-17T15:46:44.360530Z","iopub.status.idle":"2023-03-17T15:46:44.382282Z","shell.execute_reply.started":"2023-03-17T15:46:44.360484Z","shell.execute_reply":"2023-03-17T15:46:44.380955Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"X=pd.DataFrame(item for item in X)\nX","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:46:44.383643Z","iopub.execute_input":"2023-03-17T15:46:44.384043Z","iopub.status.idle":"2023-03-17T15:46:44.502498Z","shell.execute_reply.started":"2023-03-17T15:46:44.384007Z","shell.execute_reply":"2023-03-17T15:46:44.501242Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"             0         1         2         3         4         5         6   \\\n0     -0.690730  0.329341 -0.134080  0.415425  0.454634 -0.167945  0.174554   \n1     -2.330379 -0.658113  0.536605  2.150443  0.689677  1.182678 -0.353201   \n2      0.520063  0.239662  1.180394  0.867285 -0.540528 -2.207750 -0.979437   \n3     -0.738136 -0.512649 -0.711390 -0.317458 -0.977655 -0.581738  0.493948   \n4     -0.506658  0.196423  0.253468  0.417223  0.281877 -0.064884 -0.438714   \n...         ...       ...       ...       ...       ...       ...       ...   \n15995 -0.342400  0.295285  0.157408 -0.134808 -0.785437 -0.060749 -0.366170   \n15996 -0.257414 -0.294248 -0.189176  0.397777 -1.343263 -0.621654 -0.058509   \n15997 -1.144505 -0.659150 -0.604474 -0.064998 -0.129096  0.502783 -0.199842   \n15998 -0.751631  0.296736 -0.774003  0.399290 -0.527281 -0.801148 -0.488235   \n15999 -1.103161  0.172718 -0.395307 -0.635896 -0.846669  1.106682 -0.140198   \n\n             7         8         9   ...        14        15        16  \\\n0     -0.207599 -0.596090  0.294340  ... -0.480550  0.605793 -0.521619   \n1     -0.229030  1.399189  0.283600  ... -0.546779  1.708897 -0.584189   \n2     -0.271587 -0.211511 -2.132988  ... -0.044408  0.563339 -0.685260   \n3      0.336914 -0.771470  0.255439  ... -0.343707 -1.291833 -1.329201   \n4     -0.330622  0.039883 -0.449432  ... -0.379381  0.221458 -0.711619   \n...         ...       ...       ...  ...       ...       ...       ...   \n15995  0.226425 -1.288904 -1.436252  ...  0.177401  0.314736 -0.534947   \n15996  1.449236  1.098571 -1.237672  ... -0.532969 -0.613645 -0.225347   \n15997  0.365558  1.010224 -0.210563  ... -0.324224  1.044281  0.390260   \n15998  0.149862 -0.164078 -0.573758  ...  0.653547  0.402048 -0.030921   \n15999  0.492807 -0.593920  0.369576  ...  0.001920 -0.198484 -0.648987   \n\n             17        18        19        20        21        22        23  \n0      0.201898 -0.236806  0.164764  0.472434 -0.071870  1.343287  0.213478  \n1     -0.789247  1.517152  0.603452  0.247245 -0.293009 -1.387282  0.649020  \n2     -1.254488 -0.529327  0.565892 -0.600816 -0.086433  1.512429  0.425676  \n3     -0.662330 -0.630717 -0.743287  1.686997 -0.110291  0.312838 -0.250152  \n4      0.547373  0.069806  0.024889  0.372581  0.090115  0.394934  0.005271  \n...         ...       ...       ...       ...       ...       ...       ...  \n15995 -0.649126  1.073738  0.297500  0.911574 -0.519051  0.231357 -0.506635  \n15996  0.128031  1.027369 -0.188128  2.800260  0.269004  0.961868 -0.110420  \n15997  1.742683 -0.758748 -0.216324  0.789615  0.040581  0.864824  0.405349  \n15998 -0.939558 -0.707695  0.217393  1.083568 -0.121155  1.087536  0.498983  \n15999  0.234516  0.460509  0.202769  0.604329  0.433447  0.769989  1.192084  \n\n[16000 rows x 24 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.690730</td>\n      <td>0.329341</td>\n      <td>-0.134080</td>\n      <td>0.415425</td>\n      <td>0.454634</td>\n      <td>-0.167945</td>\n      <td>0.174554</td>\n      <td>-0.207599</td>\n      <td>-0.596090</td>\n      <td>0.294340</td>\n      <td>...</td>\n      <td>-0.480550</td>\n      <td>0.605793</td>\n      <td>-0.521619</td>\n      <td>0.201898</td>\n      <td>-0.236806</td>\n      <td>0.164764</td>\n      <td>0.472434</td>\n      <td>-0.071870</td>\n      <td>1.343287</td>\n      <td>0.213478</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-2.330379</td>\n      <td>-0.658113</td>\n      <td>0.536605</td>\n      <td>2.150443</td>\n      <td>0.689677</td>\n      <td>1.182678</td>\n      <td>-0.353201</td>\n      <td>-0.229030</td>\n      <td>1.399189</td>\n      <td>0.283600</td>\n      <td>...</td>\n      <td>-0.546779</td>\n      <td>1.708897</td>\n      <td>-0.584189</td>\n      <td>-0.789247</td>\n      <td>1.517152</td>\n      <td>0.603452</td>\n      <td>0.247245</td>\n      <td>-0.293009</td>\n      <td>-1.387282</td>\n      <td>0.649020</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.520063</td>\n      <td>0.239662</td>\n      <td>1.180394</td>\n      <td>0.867285</td>\n      <td>-0.540528</td>\n      <td>-2.207750</td>\n      <td>-0.979437</td>\n      <td>-0.271587</td>\n      <td>-0.211511</td>\n      <td>-2.132988</td>\n      <td>...</td>\n      <td>-0.044408</td>\n      <td>0.563339</td>\n      <td>-0.685260</td>\n      <td>-1.254488</td>\n      <td>-0.529327</td>\n      <td>0.565892</td>\n      <td>-0.600816</td>\n      <td>-0.086433</td>\n      <td>1.512429</td>\n      <td>0.425676</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.738136</td>\n      <td>-0.512649</td>\n      <td>-0.711390</td>\n      <td>-0.317458</td>\n      <td>-0.977655</td>\n      <td>-0.581738</td>\n      <td>0.493948</td>\n      <td>0.336914</td>\n      <td>-0.771470</td>\n      <td>0.255439</td>\n      <td>...</td>\n      <td>-0.343707</td>\n      <td>-1.291833</td>\n      <td>-1.329201</td>\n      <td>-0.662330</td>\n      <td>-0.630717</td>\n      <td>-0.743287</td>\n      <td>1.686997</td>\n      <td>-0.110291</td>\n      <td>0.312838</td>\n      <td>-0.250152</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.506658</td>\n      <td>0.196423</td>\n      <td>0.253468</td>\n      <td>0.417223</td>\n      <td>0.281877</td>\n      <td>-0.064884</td>\n      <td>-0.438714</td>\n      <td>-0.330622</td>\n      <td>0.039883</td>\n      <td>-0.449432</td>\n      <td>...</td>\n      <td>-0.379381</td>\n      <td>0.221458</td>\n      <td>-0.711619</td>\n      <td>0.547373</td>\n      <td>0.069806</td>\n      <td>0.024889</td>\n      <td>0.372581</td>\n      <td>0.090115</td>\n      <td>0.394934</td>\n      <td>0.005271</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15995</th>\n      <td>-0.342400</td>\n      <td>0.295285</td>\n      <td>0.157408</td>\n      <td>-0.134808</td>\n      <td>-0.785437</td>\n      <td>-0.060749</td>\n      <td>-0.366170</td>\n      <td>0.226425</td>\n      <td>-1.288904</td>\n      <td>-1.436252</td>\n      <td>...</td>\n      <td>0.177401</td>\n      <td>0.314736</td>\n      <td>-0.534947</td>\n      <td>-0.649126</td>\n      <td>1.073738</td>\n      <td>0.297500</td>\n      <td>0.911574</td>\n      <td>-0.519051</td>\n      <td>0.231357</td>\n      <td>-0.506635</td>\n    </tr>\n    <tr>\n      <th>15996</th>\n      <td>-0.257414</td>\n      <td>-0.294248</td>\n      <td>-0.189176</td>\n      <td>0.397777</td>\n      <td>-1.343263</td>\n      <td>-0.621654</td>\n      <td>-0.058509</td>\n      <td>1.449236</td>\n      <td>1.098571</td>\n      <td>-1.237672</td>\n      <td>...</td>\n      <td>-0.532969</td>\n      <td>-0.613645</td>\n      <td>-0.225347</td>\n      <td>0.128031</td>\n      <td>1.027369</td>\n      <td>-0.188128</td>\n      <td>2.800260</td>\n      <td>0.269004</td>\n      <td>0.961868</td>\n      <td>-0.110420</td>\n    </tr>\n    <tr>\n      <th>15997</th>\n      <td>-1.144505</td>\n      <td>-0.659150</td>\n      <td>-0.604474</td>\n      <td>-0.064998</td>\n      <td>-0.129096</td>\n      <td>0.502783</td>\n      <td>-0.199842</td>\n      <td>0.365558</td>\n      <td>1.010224</td>\n      <td>-0.210563</td>\n      <td>...</td>\n      <td>-0.324224</td>\n      <td>1.044281</td>\n      <td>0.390260</td>\n      <td>1.742683</td>\n      <td>-0.758748</td>\n      <td>-0.216324</td>\n      <td>0.789615</td>\n      <td>0.040581</td>\n      <td>0.864824</td>\n      <td>0.405349</td>\n    </tr>\n    <tr>\n      <th>15998</th>\n      <td>-0.751631</td>\n      <td>0.296736</td>\n      <td>-0.774003</td>\n      <td>0.399290</td>\n      <td>-0.527281</td>\n      <td>-0.801148</td>\n      <td>-0.488235</td>\n      <td>0.149862</td>\n      <td>-0.164078</td>\n      <td>-0.573758</td>\n      <td>...</td>\n      <td>0.653547</td>\n      <td>0.402048</td>\n      <td>-0.030921</td>\n      <td>-0.939558</td>\n      <td>-0.707695</td>\n      <td>0.217393</td>\n      <td>1.083568</td>\n      <td>-0.121155</td>\n      <td>1.087536</td>\n      <td>0.498983</td>\n    </tr>\n    <tr>\n      <th>15999</th>\n      <td>-1.103161</td>\n      <td>0.172718</td>\n      <td>-0.395307</td>\n      <td>-0.635896</td>\n      <td>-0.846669</td>\n      <td>1.106682</td>\n      <td>-0.140198</td>\n      <td>0.492807</td>\n      <td>-0.593920</td>\n      <td>0.369576</td>\n      <td>...</td>\n      <td>0.001920</td>\n      <td>-0.198484</td>\n      <td>-0.648987</td>\n      <td>0.234516</td>\n      <td>0.460509</td>\n      <td>0.202769</td>\n      <td>0.604329</td>\n      <td>0.433447</td>\n      <td>0.769989</td>\n      <td>1.192084</td>\n    </tr>\n  </tbody>\n</table>\n<p>16000 rows × 24 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix,accuracy_score,f1_score\n# Fitting Random Forest Classification to the Training set\nclassifier = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', random_state = 42)\nclassifier.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:46:44.504285Z","iopub.execute_input":"2023-03-17T15:46:44.504709Z","iopub.status.idle":"2023-03-17T15:46:47.364466Z","shell.execute_reply.started":"2023-03-17T15:46:44.504671Z","shell.execute_reply":"2023-03-17T15:46:47.363401Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"RandomForestClassifier(criterion='entropy', n_estimators=20, random_state=42)"},"metadata":{}}]},{"cell_type":"code","source":"X_test=pd.read_csv('../input/emotion-dataset/validation.csv')\ny_test=X_test['label']\nX_test=X_test.drop(columns=['label'])\nX_test['text']=X_test['text'].apply(lambda x:text_cleaning(x))\nX_test['text']=X_test['text'].map(lambda x:' '.join(x))\ncard2vec = [model.infer_vector((X_test.text[i].split(' '))) \n            for i in range(0,len(X_test.text))]\n#card2vec\n#Create a list of lists\nX_test= np.array(card2vec).tolist()\nX_test=pd.DataFrame(item for item in X_test)\n#set list to dataframe column\n#X_test","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:46:47.365948Z","iopub.execute_input":"2023-03-17T15:46:47.366282Z","iopub.status.idle":"2023-03-17T15:47:00.569457Z","shell.execute_reply.started":"2023-03-17T15:46:47.366253Z","shell.execute_reply":"2023-03-17T15:47:00.568142Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"y_pred = classifier.predict(X_test)\nprint('Testing accuracy %s' % accuracy_score(y_test, y_pred))\nprint('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:00.571386Z","iopub.execute_input":"2023-03-17T15:47:00.572027Z","iopub.status.idle":"2023-03-17T15:47:00.602203Z","shell.execute_reply.started":"2023-03-17T15:47:00.571987Z","shell.execute_reply":"2023-03-17T15:47:00.600952Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Testing accuracy 0.4435\nTesting F1 score: 0.387070858115705\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Word2vec implemented with tfidf","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"review_text = data['text'].apply(gensim.utils.simple_preprocess)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:00.603889Z","iopub.execute_input":"2023-03-17T15:47:00.604344Z","iopub.status.idle":"2023-03-17T15:47:01.006328Z","shell.execute_reply.started":"2023-03-17T15:47:00.604298Z","shell.execute_reply":"2023-03-17T15:47:01.005029Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = gensim.models.Word2Vec(\n    window=10,\n    min_count=1,\n    workers=4,\n)\nmodel.build_vocab(review_text)\n\nmodel.train(review_text, total_examples=model.corpus_count, epochs=model.epochs)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:01.007988Z","iopub.execute_input":"2023-03-17T15:47:01.009380Z","iopub.status.idle":"2023-03-17T15:47:02.586129Z","shell.execute_reply.started":"2023-03-17T15:47:01.009320Z","shell.execute_reply":"2023-03-17T15:47:02.585129Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"(1001242, 1362360)"},"metadata":{}}]},{"cell_type":"code","source":"review_text[6]","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:02.587548Z","iopub.execute_input":"2023-03-17T15:47:02.587957Z","iopub.status.idle":"2023-03-17T15:47:02.595667Z","shell.execute_reply.started":"2023-03-17T15:47:02.587918Z","shell.execute_reply":"2023-03-17T15:47:02.594554Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"['ive',\n 'been',\n 'taking',\n 'or',\n 'milligrams',\n 'or',\n 'times',\n 'recommended',\n 'amount',\n 'and',\n 'ive',\n 'fallen',\n 'asleep',\n 'lot',\n 'faster',\n 'but',\n 'also',\n 'feel',\n 'like',\n 'so',\n 'funny']"},"metadata":{}}]},{"cell_type":"code","source":"model.wv.most_similar('love')","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:02.597330Z","iopub.execute_input":"2023-03-17T15:47:02.597690Z","iopub.status.idle":"2023-03-17T15:47:02.616811Z","shell.execute_reply.started":"2023-03-17T15:47:02.597657Z","shell.execute_reply":"2023-03-17T15:47:02.615562Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"[('see', 0.9893202781677246),\n ('trust', 0.9871305823326111),\n ('way', 0.9835997819900513),\n ('someone', 0.9804139137268066),\n ('can', 0.9794450998306274),\n ('kick', 0.9791281223297119),\n ('important', 0.9767650961875916),\n ('sincere', 0.9766086935997009),\n ('deal', 0.9756659269332886),\n ('will', 0.9746040105819702)]"},"metadata":{}}]},{"cell_type":"code","source":"\nmod = Word2Vec(review_text, min_count=1)\n\nwords = model.wv.index_to_key\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:02.618820Z","iopub.execute_input":"2023-03-17T15:47:02.619280Z","iopub.status.idle":"2023-03-17T15:47:04.281747Z","shell.execute_reply.started":"2023-03-17T15:47:02.619237Z","shell.execute_reply":"2023-03-17T15:47:04.280527Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\n[lemmatizer.lemmatize(word ) for word in['give','take']]","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:04.283368Z","iopub.execute_input":"2023-03-17T15:47:04.283847Z","iopub.status.idle":"2023-03-17T15:47:04.293136Z","shell.execute_reply.started":"2023-03-17T15:47:04.283797Z","shell.execute_reply":"2023-03-17T15:47:04.291913Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"['give', 'take']"},"metadata":{}}]},{"cell_type":"code","source":"def sentence_to_vector(model,sentence):\n    sentence=pd.Series(data=[sentence])\n    sentence=sentence.apply(gensim.utils.simple_preprocess).tolist()\n    sentence=[word for sen in sentence for word in sen]\n    #print([model.wv.get_vector(word)  for word in sentence])\n    return np.mean([model.wv.get_vector(word)  for word in sentence] ,axis=0)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:04.294631Z","iopub.execute_input":"2023-03-17T15:47:04.295323Z","iopub.status.idle":"2023-03-17T15:47:04.302698Z","shell.execute_reply.started":"2023-03-17T15:47:04.295289Z","shell.execute_reply":"2023-03-17T15:47:04.301502Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"sentence=[[1,2],[2,0],[3,8],[4,6]]\nnp.mean([word  for word in sentence] ,axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:04.304428Z","iopub.execute_input":"2023-03-17T15:47:04.305048Z","iopub.status.idle":"2023-03-17T15:47:04.317831Z","shell.execute_reply.started":"2023-03-17T15:47:04.305002Z","shell.execute_reply":"2023-03-17T15:47:04.316921Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"array([2.5, 4. ])"},"metadata":{}}]},{"cell_type":"code","source":"model.wv.get_vector('milligrams')","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:04.318945Z","iopub.execute_input":"2023-03-17T15:47:04.319559Z","iopub.status.idle":"2023-03-17T15:47:04.335190Z","shell.execute_reply.started":"2023-03-17T15:47:04.319520Z","shell.execute_reply":"2023-03-17T15:47:04.333999Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"array([-0.00753113,  0.00119593,  0.0068093 , -0.00390576,  0.01042199,\n       -0.00567685,  0.00271028,  0.01966865, -0.00105973, -0.00383076,\n       -0.00283908, -0.00971972,  0.00429838, -0.01243212, -0.00886823,\n       -0.01454409,  0.0016936 , -0.00731454,  0.00342193, -0.01609846,\n        0.00237075,  0.00192783,  0.0071359 , -0.00618019, -0.00393996,\n        0.00917256,  0.00806945, -0.00601391,  0.00103321,  0.00606816,\n        0.00404662,  0.00211792,  0.00627145, -0.01022667, -0.00492162,\n        0.0054958 , -0.00248308, -0.0080095 ,  0.00055807, -0.01097313,\n        0.00795571, -0.00963457,  0.01036323, -0.00805018, -0.00713864,\n        0.00067541, -0.00024432,  0.00196889, -0.00681804,  0.00342068,\n        0.01511919,  0.00500399, -0.00233244,  0.00815354, -0.01792486,\n       -0.00374777,  0.00982139,  0.00687406, -0.00957618,  0.00615614,\n       -0.01090954, -0.00811507, -0.00432805, -0.0056119 , -0.01062856,\n        0.00680897, -0.00397188,  0.00442292, -0.02083229, -0.00035347,\n       -0.02037882, -0.00768713,  0.01218759, -0.0038717 ,  0.00132616,\n        0.00509005, -0.0008404 ,  0.00924566, -0.00102302,  0.00696809,\n       -0.01769133, -0.00441089,  0.00275903,  0.00159576, -0.00331585,\n       -0.00400525,  0.0023962 ,  0.01075675,  0.00264087,  0.00164927,\n        0.0064382 ,  0.00016475, -0.00012907, -0.01277302,  0.01306412,\n       -0.00215465, -0.00188874, -0.01891958,  0.01267999, -0.00503497],\n      dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"# sentence=pd.Series(data=[\"i like cookies\"])\n# vec=sentence_to_vector(model,sentence)\n# vec","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:04.336698Z","iopub.execute_input":"2023-03-17T15:47:04.337247Z","iopub.status.idle":"2023-03-17T15:47:04.345364Z","shell.execute_reply.started":"2023-03-17T15:47:04.337210Z","shell.execute_reply":"2023-03-17T15:47:04.343851Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_wv=data.copy()\ndata_wv['avg_vec']=data_wv['text'].apply(lambda x:sentence_to_vector(model,x))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:04.346502Z","iopub.execute_input":"2023-03-17T15:47:04.347246Z","iopub.status.idle":"2023-03-17T15:47:11.407745Z","shell.execute_reply.started":"2023-03-17T15:47:04.347207Z","shell.execute_reply":"2023-03-17T15:47:11.406291Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"data_wv['label']","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:11.409811Z","iopub.execute_input":"2023-03-17T15:47:11.410728Z","iopub.status.idle":"2023-03-17T15:47:11.423703Z","shell.execute_reply.started":"2023-03-17T15:47:11.410645Z","shell.execute_reply":"2023-03-17T15:47:11.421986Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"0        0\n1        0\n2        3\n3        2\n4        3\n        ..\n15995    0\n15996    0\n15997    1\n15998    3\n15999    0\nName: label, Length: 16000, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"X=data_wv['avg_vec']\ny=data_wv['label']","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:11.425477Z","iopub.execute_input":"2023-03-17T15:47:11.425970Z","iopub.status.idle":"2023-03-17T15:47:11.436188Z","shell.execute_reply.started":"2023-03-17T15:47:11.425926Z","shell.execute_reply":"2023-03-17T15:47:11.434989Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:11.437778Z","iopub.execute_input":"2023-03-17T15:47:11.438825Z","iopub.status.idle":"2023-03-17T15:47:11.458716Z","shell.execute_reply.started":"2023-03-17T15:47:11.438776Z","shell.execute_reply":"2023-03-17T15:47:11.457830Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"0        [-0.48630798, 0.21910977, -0.22812326, 0.17314745, 0.26199728, -1.1515235, 0.7476695, 1.4771295, -0.5526883, -0.3250096, -0.015832974, -0.98798925, -0.24744533, 0.42208, -0.035409477, -0.4508495, 0.33316934, -0.3306909, -0.12220719, -1.0841125, 0.5408724, 0.49886084, 0.6442618, -0.0174935, -0.031155542, -0.19816792, -0.40465045, -0.03122955, -0.4814837, -0.4148896, 0.7666363, 0.09302827, 0.45717022, -0.44394603, -0.4371657, 0.44208583, 0.53838116, -0.34836543, -0.016219543, -0.80225176, -0.053418178, -0.11506363, -0.22092457, 0.043563295, 0.33603752, -0.013722122, -0.38574922, -0.014747317, 0.60559654, 0.53271174, 0.24595173, 0.10969442, 0.0021367471, -0.18206452, 0.118540145, 0.010043182, 0.39718768, -0.25545254, -0.32835585, 0.44345984, 0.23224966, 0.002979651, 0.20474528, -0.0062864535, -0.71497655, 0.5412827, 0.24362914, 0.90707856, -0.99104863, 0.7776703, -0.27310017, 0.23115973, 0.80750465, -0.11494819, 0.6535348, -0.035203844, 0.1900326, -0.06395602, -0.5032592, 0.013270634,...\n1        [-0.39158428, 0.25744992, -0.06449004, 0.113982, 0.40768614, -0.96942794, 0.6843294, 1.5401766, -0.48025376, -0.28657848, -0.09025202, -1.0129472, 0.05585193, 0.48996335, 0.04607723, -0.59766144, 0.5727804, -0.47789326, -0.18463324, -1.3096673, 0.47011274, 0.26463506, 0.98661345, -0.20360318, -0.07839657, -0.03995726, -0.3925358, 0.0051434096, -0.48020864, -0.090962425, 0.9986661, 0.03990092, 0.38603368, -0.34124047, -0.43345696, 0.5117506, 0.48017606, -0.43780255, -0.09337862, -0.88521326, 0.036836993, -0.28053573, -0.053948253, -0.075595304, 0.38356, -0.11230619, -0.37294403, -0.46759582, 0.49270034, 0.36084437, 0.51225555, -0.09341021, -0.22377472, -0.3299757, -0.201733, -0.11411552, 0.25864285, -0.09977677, -0.3517064, 0.2604019, 0.13773784, -0.30091953, 0.27029166, 0.258043, -0.80095845, 0.7462921, 0.4120462, 0.95329124, -1.1513442, 1.0414513, -0.5649393, 0.16456516, 0.751174, 0.17102551, 0.59126806, -0.14311387, 0.24355173, 0.02143898, -0.45647177, 0.30066752, -0.56500727, -0...\n2        [-0.33733132, 0.13734338, -0.14021538, 0.08967267, 0.29389355, -0.8133904, 0.52559525, 1.1818485, -0.41826707, -0.19896322, -0.05068195, -0.7581199, 0.00012550969, 0.44121563, 0.040724542, -0.30039608, 0.30475658, -0.23319648, -0.16053717, -0.93621534, 0.37406263, 0.3019282, 0.58174145, -0.10542362, -0.03906358, -0.12216604, -0.25253394, -0.11824668, -0.37781152, -0.15138777, 0.7442418, 0.043065444, 0.37361717, -0.39509246, -0.3624038, 0.4204685, 0.3221134, -0.22062013, -0.0014498141, -0.7027235, 0.014712677, -0.20822431, -0.1307196, 0.062047243, 0.3228325, -0.007185338, -0.39666238, -0.17895532, 0.536799, 0.3388565, 0.43513092, -0.013524206, -0.14659838, -0.14596497, -0.16255483, 0.014908681, 0.3597467, -0.09507375, -0.28485778, 0.26651055, 0.18261373, -0.03019157, 0.19134524, 0.13667603, -0.5766244, 0.54396605, 0.25641552, 0.6804688, -0.7806755, 0.6754629, -0.31322312, 0.1366288, 0.5428126, -0.008540931, 0.54045725, 0.0038292184, 0.20200513, -0.0085939, -0.3916546, 0.09245719, -0...\n3        [-0.38176668, 0.27156585, -0.14845926, 0.13786942, 0.45145792, -1.0611572, 0.69424593, 1.6297647, -0.49265254, -0.36452478, -0.029867256, -1.0194311, -0.019792031, 0.49207506, 0.10658194, -0.40378198, 0.59624076, -0.5497255, -0.23894385, -1.4425901, 0.54537296, 0.36245707, 0.95623857, -0.14854261, -0.13167234, -0.041059162, -0.3793306, -0.03696253, -0.4421376, -0.085821025, 1.0200315, 0.10132024, 0.49022093, -0.4577257, -0.44080028, 0.69437045, 0.56787974, -0.490268, -0.13094006, -0.93502766, 0.076561876, -0.3660874, -0.024701104, -0.051906817, 0.45586994, -0.13922668, -0.5242547, -0.464443, 0.6438687, 0.44393933, 0.5459973, -0.08042359, -0.20298626, -0.26225197, -0.14473467, -0.0079719825, 0.2985735, -0.10523115, -0.27971074, 0.26960254, 0.19769754, -0.20727716, 0.3155318, 0.29570606, -0.8070275, 0.7511853, 0.38369897, 0.99561614, -1.1688071, 1.012742, -0.5218062, 0.123471774, 0.71731454, 0.1537981, 0.60749185, -0.1793471, 0.2814676, 0.03405161, -0.4687637, 0.25489658, -0.6159354,...\n4        [-0.3296868, 0.21729328, -0.061801612, -0.057830077, 0.14760223, -0.9299383, 0.5603515, 1.345035, -0.7844262, -0.43968585, -0.06908302, -0.70432234, -0.22270733, 0.32799467, 0.11473242, -0.21155979, 0.16073145, -0.50133187, -0.37824047, -1.2069288, 0.52242315, 0.3785697, 0.76889753, 0.29159388, -0.25396505, 0.006968845, -0.2820247, -0.3808563, -0.22669244, 0.27108994, 0.951113, -0.12983756, 0.30127886, -0.918081, -0.4140688, 1.1213063, 0.3556752, -0.046848465, -0.026313148, -0.91271275, 0.4974779, -0.7488585, -0.19205351, 0.04017096, 0.25974187, -0.18022859, -0.66642106, -0.056016076, 0.9078736, 0.37844825, 0.815717, -0.29654682, 0.09665126, 0.38467512, -0.38840887, 0.16688101, 0.9717054, -0.06364202, -0.4432108, 0.057764545, 0.3054838, 0.15823188, 0.24634326, 0.10115824, -0.50630707, 0.65605885, 0.44519147, 0.7118339, -1.1697227, 0.57001317, -0.31098816, 0.12664811, 0.5265983, 0.04391423, 0.47464648, 0.07265351, -0.05555463, 0.24919237, -0.33339548, 0.18893789, -0.40935162, -0.194...\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n15995    [-0.29108483, 0.2270115, -0.06385076, -0.038887095, 0.44220552, -0.84820336, 0.70667535, 1.431567, -0.46283868, -0.1703664, -0.041854344, -0.9400908, 0.07893776, 0.17454083, 0.009574299, -0.5968859, 0.5910134, -0.48892257, -0.35722107, -1.4144256, 0.5217404, 0.24347089, 1.0490032, -0.13025087, -0.20624092, -0.058864933, -0.25958195, -0.10430526, -0.38847646, 0.04823103, 1.0056273, -0.041818906, 0.386006, -0.38660187, -0.4110311, 0.5204777, 0.24639912, -0.5263659, 0.032063987, -0.958411, 0.16658868, -0.47507727, 0.109753385, -0.046786916, 0.39244914, -0.037656665, -0.51303434, -0.4558383, 0.4385927, 0.46272746, 0.597538, -0.062405605, -0.14724462, -0.34440848, -0.24455778, -0.034959864, 0.37773538, -0.12661931, -0.3168669, 0.24079323, 0.10504759, -0.3276883, 0.22170913, 0.21027054, -0.64329845, 0.8287492, 0.36676845, 0.9536668, -1.3109013, 0.95988417, -0.670374, 0.13408819, 0.68677306, 0.2016337, 0.52193546, -0.25277284, 0.27163646, 0.11438558, -0.45753428, 0.26230857, -0.6769849, 0...\n15996    [-0.31143185, 0.22915553, -0.038885802, 0.11308843, 0.3316916, -0.86147785, 0.4915195, 1.2423267, -0.44377393, -0.32397106, -0.03699532, -0.7487248, -0.042132802, 0.43594855, 0.107513, -0.31198412, 0.36992222, -0.34502095, -0.21887553, -1.0647814, 0.41534916, 0.27689743, 0.68192303, -0.08872065, -0.085618295, -0.0060211387, -0.32344693, -0.06383263, -0.31033868, 0.002639122, 0.79384655, 0.037243366, 0.37453377, -0.4272157, -0.33810803, 0.5765438, 0.40787834, -0.25888208, -0.027779598, -0.74421954, 0.1093685, -0.31832176, -0.08611263, -0.018262181, 0.28838405, -0.10433806, -0.43720683, -0.30451572, 0.554101, 0.3681256, 0.45329982, -0.10163036, -0.123415865, -0.076601766, -0.16266568, 0.031447247, 0.365812, -0.07100524, -0.2175609, 0.2194691, 0.14641474, -0.06916516, 0.17204565, 0.14359416, -0.60299134, 0.5698088, 0.32303286, 0.67709064, -0.8567537, 0.7632433, -0.35611987, 0.08232856, 0.51792586, 0.11301812, 0.4812982, -0.08014382, 0.18891023, 0.083251335, -0.31353673, 0.16915211, -0...\n15997    [-0.47143236, 0.27949724, -0.08423929, 0.21839757, 0.313554, -0.99119186, 0.66746193, 1.467602, -0.46419662, -0.30707517, -0.014134529, -0.9213268, -0.101221934, 0.3871838, -0.027987655, -0.5288401, 0.45645934, -0.4449625, -0.16930827, -1.1184764, 0.50594485, 0.31910813, 0.873408, -0.03882413, -0.029010648, -0.060360022, -0.39450505, -0.023737023, -0.4086658, -0.150945, 0.81931436, 0.0042218133, 0.3533767, -0.38176578, -0.41809207, 0.47522917, 0.4593287, -0.39120263, -0.035074696, -0.7900433, 0.0613917, -0.24530904, -0.07178186, -0.08797781, 0.30810064, -0.0011190284, -0.3906594, -0.22947471, 0.54740393, 0.40749606, 0.4057269, -0.02311204, -0.08420207, -0.19405755, -0.08570023, -0.067886695, 0.36105034, -0.16177094, -0.3429871, 0.30130517, 0.1550861, -0.15990289, 0.15589052, 0.11863843, -0.74174434, 0.5858223, 0.3545569, 0.88802195, -1.0656197, 0.8647401, -0.47836643, 0.1626369, 0.72445685, 0.0030048057, 0.542511, -0.16612476, 0.179049, 0.029900575, -0.41802907, 0.14857264, -0.5286...\n15998    [-0.32867056, 0.26515424, -0.124120146, 0.07589254, 0.3769906, -1.0902785, 0.6566797, 1.5440828, -0.54464316, -0.32821625, -0.08139945, -0.95888466, -0.09361978, 0.473438, 0.07544829, -0.33103368, 0.49065104, -0.46712887, -0.24711178, -1.4570569, 0.5644079, 0.3849414, 0.8355936, -0.11725371, -0.13390993, -0.022189265, -0.38103893, -0.06906972, -0.4126915, -0.052693054, 1.0001441, 0.10229003, 0.54224426, -0.53211856, -0.4222064, 0.6813678, 0.5150939, -0.42456752, -0.0712539, -0.94483185, 0.08466987, -0.41632128, -0.13595152, 0.07986937, 0.4248127, -0.14879312, -0.56497467, -0.37446842, 0.7051477, 0.5562393, 0.55278, -0.056124914, -0.15107916, -0.18979658, -0.15286084, 0.089925684, 0.39317828, -0.13396786, -0.26073295, 0.3437481, 0.20334996, -0.14781526, 0.28761083, 0.20077701, -0.70691323, 0.7367084, 0.347165, 0.9239454, -1.108417, 0.9823566, -0.45697758, 0.13155521, 0.6750752, 0.13307942, 0.6029686, -0.11083281, 0.28265712, 0.026958345, -0.41458234, 0.18735667, -0.5487883, -0.07137...\n15999    [-0.43899146, 0.26209772, -0.25985548, 0.12912478, 0.41537046, -1.3950359, 0.84025854, 1.7806861, -0.62366766, -0.46889374, -0.105627775, -1.1940508, -0.18693787, 0.77245927, 0.11978431, -0.37327215, 0.5069207, -0.41194323, -0.040601287, -1.428756, 0.47682247, 0.4345215, 0.69185257, -0.30811518, -0.079300344, -0.22837509, -0.49980032, 0.059669986, -0.5368147, -0.47324374, 0.98753583, 0.24112064, 0.60644627, -0.48864976, -0.4571205, 0.6158448, 0.77271724, -0.4395973, -0.16581176, -0.8979496, -0.11494949, -0.12354172, -0.32294643, 0.08294293, 0.5171018, -0.22030479, -0.43498266, -0.29967535, 0.5916839, 0.5767716, 0.3620845, 0.09165454, -0.19754729, -0.2619959, 0.058838006, 0.0029630363, 0.25639975, -0.21486211, -0.23499489, 0.40447122, 0.25162455, -0.12187496, 0.45522735, 0.1729013, -0.8164776, 0.69893223, 0.291951, 1.0325595, -1.0602647, 1.0121012, -0.26761553, 0.2026663, 0.92208475, 0.098924756, 0.7644394, 0.064472586, 0.20715708, -0.20183335, -0.60473734, 0.15627426, -0.36842334, ...\nName: avg_vec, Length: 16000, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"X=pd.DataFrame(item for item in X)\nX","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:11.460027Z","iopub.execute_input":"2023-03-17T15:47:11.461152Z","iopub.status.idle":"2023-03-17T15:47:12.276223Z","shell.execute_reply.started":"2023-03-17T15:47:11.461113Z","shell.execute_reply":"2023-03-17T15:47:12.274986Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"             0         1         2         3         4         5         6   \\\n0     -0.486308  0.219110 -0.228123  0.173147  0.261997 -1.151523  0.747670   \n1     -0.391584  0.257450 -0.064490  0.113982  0.407686 -0.969428  0.684329   \n2     -0.337331  0.137343 -0.140215  0.089673  0.293894 -0.813390  0.525595   \n3     -0.381767  0.271566 -0.148459  0.137869  0.451458 -1.061157  0.694246   \n4     -0.329687  0.217293 -0.061802 -0.057830  0.147602 -0.929938  0.560351   \n...         ...       ...       ...       ...       ...       ...       ...   \n15995 -0.291085  0.227012 -0.063851 -0.038887  0.442206 -0.848203  0.706675   \n15996 -0.311432  0.229156 -0.038886  0.113088  0.331692 -0.861478  0.491520   \n15997 -0.471432  0.279497 -0.084239  0.218398  0.313554 -0.991192  0.667462   \n15998 -0.328671  0.265154 -0.124120  0.075893  0.376991 -1.090279  0.656680   \n15999 -0.438991  0.262098 -0.259855  0.129125  0.415370 -1.395036  0.840259   \n\n             7         8         9   ...        90        91        92  \\\n0      1.477129 -0.552688 -0.325010  ...  0.799637  0.449649  0.405128   \n1      1.540177 -0.480254 -0.286578  ...  0.846091  0.280938  0.505993   \n2      1.181849 -0.418267 -0.198963  ...  0.580888  0.321140  0.297546   \n3      1.629765 -0.492653 -0.364525  ...  0.848089  0.320445  0.562382   \n4      1.345035 -0.784426 -0.439686  ...  0.744084  0.603039  0.067171   \n...         ...       ...       ...  ...       ...       ...       ...   \n15995  1.431567 -0.462839 -0.170366  ...  0.687409  0.214555  0.614739   \n15996  1.242327 -0.443774 -0.323971  ...  0.635792  0.267810  0.374582   \n15997  1.467602 -0.464197 -0.307075  ...  0.742918  0.304390  0.425799   \n15998  1.544083 -0.544643 -0.328216  ...  0.820544  0.341854  0.531906   \n15999  1.780686 -0.623668 -0.468894  ...  1.000387  0.443860  0.444061   \n\n             93        94        95        96        97        98        99  \n0      0.112862  1.431624  0.612421 -0.053955 -0.352584  0.246445  0.380044  \n1     -0.024976  1.435132  0.579495  0.267001 -0.203620  0.198355  0.239889  \n2      0.032790  1.074596  0.490662  0.143570 -0.270207  0.086250  0.270866  \n3      0.102610  1.425847  0.562244  0.236209 -0.213073  0.171968  0.312961  \n4      0.030678  1.053255  0.554806 -0.086623 -0.928096  0.258335  0.351046  \n...         ...       ...       ...       ...       ...       ...       ...  \n15995 -0.007121  1.399390  0.424677  0.247844 -0.374669  0.295424  0.162075  \n15996  0.028973  0.987412  0.444557  0.122935 -0.302986  0.104699  0.283196  \n15997  0.026088  1.353148  0.574622  0.090661 -0.366795  0.249970  0.377242  \n15998  0.044658  1.286869  0.488313  0.176767 -0.331299  0.153301  0.318734  \n15999  0.155388  1.585141  0.696021  0.108515 -0.027772  0.108769  0.240682  \n\n[16000 rows x 100 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n      <th>95</th>\n      <th>96</th>\n      <th>97</th>\n      <th>98</th>\n      <th>99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.486308</td>\n      <td>0.219110</td>\n      <td>-0.228123</td>\n      <td>0.173147</td>\n      <td>0.261997</td>\n      <td>-1.151523</td>\n      <td>0.747670</td>\n      <td>1.477129</td>\n      <td>-0.552688</td>\n      <td>-0.325010</td>\n      <td>...</td>\n      <td>0.799637</td>\n      <td>0.449649</td>\n      <td>0.405128</td>\n      <td>0.112862</td>\n      <td>1.431624</td>\n      <td>0.612421</td>\n      <td>-0.053955</td>\n      <td>-0.352584</td>\n      <td>0.246445</td>\n      <td>0.380044</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.391584</td>\n      <td>0.257450</td>\n      <td>-0.064490</td>\n      <td>0.113982</td>\n      <td>0.407686</td>\n      <td>-0.969428</td>\n      <td>0.684329</td>\n      <td>1.540177</td>\n      <td>-0.480254</td>\n      <td>-0.286578</td>\n      <td>...</td>\n      <td>0.846091</td>\n      <td>0.280938</td>\n      <td>0.505993</td>\n      <td>-0.024976</td>\n      <td>1.435132</td>\n      <td>0.579495</td>\n      <td>0.267001</td>\n      <td>-0.203620</td>\n      <td>0.198355</td>\n      <td>0.239889</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.337331</td>\n      <td>0.137343</td>\n      <td>-0.140215</td>\n      <td>0.089673</td>\n      <td>0.293894</td>\n      <td>-0.813390</td>\n      <td>0.525595</td>\n      <td>1.181849</td>\n      <td>-0.418267</td>\n      <td>-0.198963</td>\n      <td>...</td>\n      <td>0.580888</td>\n      <td>0.321140</td>\n      <td>0.297546</td>\n      <td>0.032790</td>\n      <td>1.074596</td>\n      <td>0.490662</td>\n      <td>0.143570</td>\n      <td>-0.270207</td>\n      <td>0.086250</td>\n      <td>0.270866</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.381767</td>\n      <td>0.271566</td>\n      <td>-0.148459</td>\n      <td>0.137869</td>\n      <td>0.451458</td>\n      <td>-1.061157</td>\n      <td>0.694246</td>\n      <td>1.629765</td>\n      <td>-0.492653</td>\n      <td>-0.364525</td>\n      <td>...</td>\n      <td>0.848089</td>\n      <td>0.320445</td>\n      <td>0.562382</td>\n      <td>0.102610</td>\n      <td>1.425847</td>\n      <td>0.562244</td>\n      <td>0.236209</td>\n      <td>-0.213073</td>\n      <td>0.171968</td>\n      <td>0.312961</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.329687</td>\n      <td>0.217293</td>\n      <td>-0.061802</td>\n      <td>-0.057830</td>\n      <td>0.147602</td>\n      <td>-0.929938</td>\n      <td>0.560351</td>\n      <td>1.345035</td>\n      <td>-0.784426</td>\n      <td>-0.439686</td>\n      <td>...</td>\n      <td>0.744084</td>\n      <td>0.603039</td>\n      <td>0.067171</td>\n      <td>0.030678</td>\n      <td>1.053255</td>\n      <td>0.554806</td>\n      <td>-0.086623</td>\n      <td>-0.928096</td>\n      <td>0.258335</td>\n      <td>0.351046</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15995</th>\n      <td>-0.291085</td>\n      <td>0.227012</td>\n      <td>-0.063851</td>\n      <td>-0.038887</td>\n      <td>0.442206</td>\n      <td>-0.848203</td>\n      <td>0.706675</td>\n      <td>1.431567</td>\n      <td>-0.462839</td>\n      <td>-0.170366</td>\n      <td>...</td>\n      <td>0.687409</td>\n      <td>0.214555</td>\n      <td>0.614739</td>\n      <td>-0.007121</td>\n      <td>1.399390</td>\n      <td>0.424677</td>\n      <td>0.247844</td>\n      <td>-0.374669</td>\n      <td>0.295424</td>\n      <td>0.162075</td>\n    </tr>\n    <tr>\n      <th>15996</th>\n      <td>-0.311432</td>\n      <td>0.229156</td>\n      <td>-0.038886</td>\n      <td>0.113088</td>\n      <td>0.331692</td>\n      <td>-0.861478</td>\n      <td>0.491520</td>\n      <td>1.242327</td>\n      <td>-0.443774</td>\n      <td>-0.323971</td>\n      <td>...</td>\n      <td>0.635792</td>\n      <td>0.267810</td>\n      <td>0.374582</td>\n      <td>0.028973</td>\n      <td>0.987412</td>\n      <td>0.444557</td>\n      <td>0.122935</td>\n      <td>-0.302986</td>\n      <td>0.104699</td>\n      <td>0.283196</td>\n    </tr>\n    <tr>\n      <th>15997</th>\n      <td>-0.471432</td>\n      <td>0.279497</td>\n      <td>-0.084239</td>\n      <td>0.218398</td>\n      <td>0.313554</td>\n      <td>-0.991192</td>\n      <td>0.667462</td>\n      <td>1.467602</td>\n      <td>-0.464197</td>\n      <td>-0.307075</td>\n      <td>...</td>\n      <td>0.742918</td>\n      <td>0.304390</td>\n      <td>0.425799</td>\n      <td>0.026088</td>\n      <td>1.353148</td>\n      <td>0.574622</td>\n      <td>0.090661</td>\n      <td>-0.366795</td>\n      <td>0.249970</td>\n      <td>0.377242</td>\n    </tr>\n    <tr>\n      <th>15998</th>\n      <td>-0.328671</td>\n      <td>0.265154</td>\n      <td>-0.124120</td>\n      <td>0.075893</td>\n      <td>0.376991</td>\n      <td>-1.090279</td>\n      <td>0.656680</td>\n      <td>1.544083</td>\n      <td>-0.544643</td>\n      <td>-0.328216</td>\n      <td>...</td>\n      <td>0.820544</td>\n      <td>0.341854</td>\n      <td>0.531906</td>\n      <td>0.044658</td>\n      <td>1.286869</td>\n      <td>0.488313</td>\n      <td>0.176767</td>\n      <td>-0.331299</td>\n      <td>0.153301</td>\n      <td>0.318734</td>\n    </tr>\n    <tr>\n      <th>15999</th>\n      <td>-0.438991</td>\n      <td>0.262098</td>\n      <td>-0.259855</td>\n      <td>0.129125</td>\n      <td>0.415370</td>\n      <td>-1.395036</td>\n      <td>0.840259</td>\n      <td>1.780686</td>\n      <td>-0.623668</td>\n      <td>-0.468894</td>\n      <td>...</td>\n      <td>1.000387</td>\n      <td>0.443860</td>\n      <td>0.444061</td>\n      <td>0.155388</td>\n      <td>1.585141</td>\n      <td>0.696021</td>\n      <td>0.108515</td>\n      <td>-0.027772</td>\n      <td>0.108769</td>\n      <td>0.240682</td>\n    </tr>\n  </tbody>\n</table>\n<p>16000 rows × 100 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix,accuracy_score,f1_score\n# Fitting Random Forest Classification to the Training set\nclassifier = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', random_state = 42)\nclassifier.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:12.277923Z","iopub.execute_input":"2023-03-17T15:47:12.278306Z","iopub.status.idle":"2023-03-17T15:47:20.681346Z","shell.execute_reply.started":"2023-03-17T15:47:12.278274Z","shell.execute_reply":"2023-03-17T15:47:20.679991Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"RandomForestClassifier(criterion='entropy', n_estimators=20, random_state=42)"},"metadata":{}}]},{"cell_type":"code","source":"X_test=pd.read_csv('../input/emotion-dataset/validation.csv')\ny_test=X_test['label']\nX_test=X_test.drop(columns=['label'])\nX_test=X_test['text'].apply(gensim.utils.simple_preprocess)\ndocuments=[]\nfor x in X_test:\n    document = [word for word in x if word in model.wv.index_to_key]\n    documents.append(document)\nlen(documents)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:20.682833Z","iopub.execute_input":"2023-03-17T15:47:20.683249Z","iopub.status.idle":"2023-03-17T15:47:21.606348Z","shell.execute_reply.started":"2023-03-17T15:47:20.683204Z","shell.execute_reply":"2023-03-17T15:47:21.605130Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"2000"},"metadata":{}}]},{"cell_type":"code","source":"counter = 0\nfor i in range (0,len(documents)):\n    if documents[i] == []:\n        counter += 1\nprint(counter)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:21.607835Z","iopub.execute_input":"2023-03-17T15:47:21.608319Z","iopub.status.idle":"2023-03-17T15:47:21.617196Z","shell.execute_reply.started":"2023-03-17T15:47:21.608271Z","shell.execute_reply":"2023-03-17T15:47:21.615655Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"}]},{"cell_type":"code","source":"documents=[' '.join(i) for i in documents]\ndocuments=pd.Series(data=documents)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:21.626923Z","iopub.execute_input":"2023-03-17T15:47:21.627340Z","iopub.status.idle":"2023-03-17T15:47:21.635132Z","shell.execute_reply.started":"2023-03-17T15:47:21.627307Z","shell.execute_reply":"2023-03-17T15:47:21.633754Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"documents","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:21.636768Z","iopub.execute_input":"2023-03-17T15:47:21.637444Z","iopub.status.idle":"2023-03-17T15:47:21.651032Z","shell.execute_reply.started":"2023-03-17T15:47:21.637409Z","shell.execute_reply":"2023-03-17T15:47:21.649695Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"0                                                                                                                           im feeling quite sad and sorry for myself but ill snap out of it soon\n1                                                                                                                                feel like am still looking at blank canvas blank pieces of paper\n2                                                                                                                                                                              feel like faithful\n3                                                                                                                                                                 am just feeling cranky and blue\n4                                                                                                                                                     can have for treat or if am feeling festive\n                                                                                                  ...                                                                                            \n1995    im having examination tomorrow in the morning im quite well prepared for the coming exam and somehow feel numb towards exam because in life there is much more important things than exam\n1996                    constantly worry about their fight against nature as they push the limits of their inner bodies for the determination of their outer existence but somehow feel reassured\n1997                                                                                                               feel its important to share this info for those that experience the same thing\n1998                                                                                      truly feel that if you are passionate enough about something and stay true to yourself you will succeed\n1999                                                                                                                         feel like just wanna buy any cute make up see online or even the one\nLength: 2000, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"X_test=documents.apply(lambda x:sentence_to_vector(model,x))\nX_test=pd.DataFrame(item for item in X_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:21.652510Z","iopub.execute_input":"2023-03-17T15:47:21.652928Z","iopub.status.idle":"2023-03-17T15:47:22.633834Z","shell.execute_reply.started":"2023-03-17T15:47:21.652885Z","shell.execute_reply":"2023-03-17T15:47:22.632642Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"X_test","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:22.635428Z","iopub.execute_input":"2023-03-17T15:47:22.635766Z","iopub.status.idle":"2023-03-17T15:47:22.668323Z","shell.execute_reply.started":"2023-03-17T15:47:22.635736Z","shell.execute_reply":"2023-03-17T15:47:22.667155Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"            0         1         2         3         4         5         6   \\\n0    -0.378675  0.278221 -0.035383  0.062942  0.343190 -0.979123  0.609473   \n1    -0.336314  0.248537 -0.014185  0.099632  0.338417 -0.821691  0.460539   \n2    -0.473594  0.233770 -0.198576  0.085931  0.315825 -1.285671  0.788428   \n3    -0.347664  0.235042 -0.023275  0.016278  0.241170 -0.964649  0.592445   \n4    -0.466940  0.291131 -0.072703  0.041212  0.424903 -1.131771  0.734799   \n...        ...       ...       ...       ...       ...       ...       ...   \n1995 -0.359107  0.293745 -0.053198  0.136726  0.402613 -0.869984  0.519810   \n1996 -0.402960  0.330959 -0.018096  0.236033  0.400845 -0.735179  0.519109   \n1997 -0.429883  0.264008 -0.131206  0.175064  0.434708 -0.955389  0.728992   \n1998 -0.495823  0.285021 -0.201532  0.203451  0.532686 -1.204964  0.804856   \n1999 -0.461833  0.290106 -0.094030  0.175009  0.419192 -1.051559  0.702931   \n\n            7         8         9   ...        90        91        92  \\\n0     1.526425 -0.573149 -0.339812  ...  0.789358  0.361212  0.414027   \n1     1.264634 -0.460728 -0.322052  ...  0.678921  0.319609  0.421199   \n2     1.611873 -0.621977 -0.438458  ...  0.921546  0.446135  0.485798   \n3     1.470401 -0.660358 -0.382737  ...  0.778110  0.441806  0.255891   \n4     1.606942 -0.612437 -0.359122  ...  0.926690  0.469775  0.532073   \n...        ...       ...       ...  ...       ...       ...       ...   \n1995  1.372793 -0.402585 -0.311904  ...  0.678492  0.246475  0.543236   \n1996  1.193744 -0.253581 -0.257388  ...  0.558381  0.137599  0.456042   \n1997  1.522837 -0.396136 -0.271497  ...  0.713310  0.221968  0.548558   \n1998  1.632865 -0.424039 -0.310546  ...  0.860558  0.284104  0.503761   \n1999  1.599988 -0.461818 -0.354757  ...  0.788346  0.299937  0.548200   \n\n            93        94        95        96        97        98        99  \n0    -0.016464  1.290950  0.543654  0.189564 -0.441774  0.207100  0.296348  \n1     0.017774  1.101234  0.474653  0.170714 -0.364569  0.141558  0.319610  \n2     0.112867  1.608201  0.661879 -0.010650 -0.254598  0.226181  0.268113  \n3    -0.017993  1.172644  0.565401  0.090044 -0.637288  0.215591  0.304137  \n4     0.000278  1.574524  0.624532  0.229672 -0.239826  0.163114  0.209363  \n...        ...       ...       ...       ...       ...       ...       ...  \n1995  0.053936  1.209673  0.468925  0.169067 -0.303890  0.157168  0.322682  \n1996  0.006615  1.130699  0.406514  0.235922 -0.101245  0.149098  0.306883  \n1997  0.063899  1.445132  0.527112  0.255640 -0.144263  0.227086  0.275860  \n1998  0.064216  1.566406  0.640111  0.259832  0.176842 -0.001190  0.195558  \n1999  0.047196  1.474246  0.553182  0.250260 -0.141310  0.191782  0.260403  \n\n[2000 rows x 100 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n      <th>95</th>\n      <th>96</th>\n      <th>97</th>\n      <th>98</th>\n      <th>99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.378675</td>\n      <td>0.278221</td>\n      <td>-0.035383</td>\n      <td>0.062942</td>\n      <td>0.343190</td>\n      <td>-0.979123</td>\n      <td>0.609473</td>\n      <td>1.526425</td>\n      <td>-0.573149</td>\n      <td>-0.339812</td>\n      <td>...</td>\n      <td>0.789358</td>\n      <td>0.361212</td>\n      <td>0.414027</td>\n      <td>-0.016464</td>\n      <td>1.290950</td>\n      <td>0.543654</td>\n      <td>0.189564</td>\n      <td>-0.441774</td>\n      <td>0.207100</td>\n      <td>0.296348</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.336314</td>\n      <td>0.248537</td>\n      <td>-0.014185</td>\n      <td>0.099632</td>\n      <td>0.338417</td>\n      <td>-0.821691</td>\n      <td>0.460539</td>\n      <td>1.264634</td>\n      <td>-0.460728</td>\n      <td>-0.322052</td>\n      <td>...</td>\n      <td>0.678921</td>\n      <td>0.319609</td>\n      <td>0.421199</td>\n      <td>0.017774</td>\n      <td>1.101234</td>\n      <td>0.474653</td>\n      <td>0.170714</td>\n      <td>-0.364569</td>\n      <td>0.141558</td>\n      <td>0.319610</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.473594</td>\n      <td>0.233770</td>\n      <td>-0.198576</td>\n      <td>0.085931</td>\n      <td>0.315825</td>\n      <td>-1.285671</td>\n      <td>0.788428</td>\n      <td>1.611873</td>\n      <td>-0.621977</td>\n      <td>-0.438458</td>\n      <td>...</td>\n      <td>0.921546</td>\n      <td>0.446135</td>\n      <td>0.485798</td>\n      <td>0.112867</td>\n      <td>1.608201</td>\n      <td>0.661879</td>\n      <td>-0.010650</td>\n      <td>-0.254598</td>\n      <td>0.226181</td>\n      <td>0.268113</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.347664</td>\n      <td>0.235042</td>\n      <td>-0.023275</td>\n      <td>0.016278</td>\n      <td>0.241170</td>\n      <td>-0.964649</td>\n      <td>0.592445</td>\n      <td>1.470401</td>\n      <td>-0.660358</td>\n      <td>-0.382737</td>\n      <td>...</td>\n      <td>0.778110</td>\n      <td>0.441806</td>\n      <td>0.255891</td>\n      <td>-0.017993</td>\n      <td>1.172644</td>\n      <td>0.565401</td>\n      <td>0.090044</td>\n      <td>-0.637288</td>\n      <td>0.215591</td>\n      <td>0.304137</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.466940</td>\n      <td>0.291131</td>\n      <td>-0.072703</td>\n      <td>0.041212</td>\n      <td>0.424903</td>\n      <td>-1.131771</td>\n      <td>0.734799</td>\n      <td>1.606942</td>\n      <td>-0.612437</td>\n      <td>-0.359122</td>\n      <td>...</td>\n      <td>0.926690</td>\n      <td>0.469775</td>\n      <td>0.532073</td>\n      <td>0.000278</td>\n      <td>1.574524</td>\n      <td>0.624532</td>\n      <td>0.229672</td>\n      <td>-0.239826</td>\n      <td>0.163114</td>\n      <td>0.209363</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>-0.359107</td>\n      <td>0.293745</td>\n      <td>-0.053198</td>\n      <td>0.136726</td>\n      <td>0.402613</td>\n      <td>-0.869984</td>\n      <td>0.519810</td>\n      <td>1.372793</td>\n      <td>-0.402585</td>\n      <td>-0.311904</td>\n      <td>...</td>\n      <td>0.678492</td>\n      <td>0.246475</td>\n      <td>0.543236</td>\n      <td>0.053936</td>\n      <td>1.209673</td>\n      <td>0.468925</td>\n      <td>0.169067</td>\n      <td>-0.303890</td>\n      <td>0.157168</td>\n      <td>0.322682</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>-0.402960</td>\n      <td>0.330959</td>\n      <td>-0.018096</td>\n      <td>0.236033</td>\n      <td>0.400845</td>\n      <td>-0.735179</td>\n      <td>0.519109</td>\n      <td>1.193744</td>\n      <td>-0.253581</td>\n      <td>-0.257388</td>\n      <td>...</td>\n      <td>0.558381</td>\n      <td>0.137599</td>\n      <td>0.456042</td>\n      <td>0.006615</td>\n      <td>1.130699</td>\n      <td>0.406514</td>\n      <td>0.235922</td>\n      <td>-0.101245</td>\n      <td>0.149098</td>\n      <td>0.306883</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>-0.429883</td>\n      <td>0.264008</td>\n      <td>-0.131206</td>\n      <td>0.175064</td>\n      <td>0.434708</td>\n      <td>-0.955389</td>\n      <td>0.728992</td>\n      <td>1.522837</td>\n      <td>-0.396136</td>\n      <td>-0.271497</td>\n      <td>...</td>\n      <td>0.713310</td>\n      <td>0.221968</td>\n      <td>0.548558</td>\n      <td>0.063899</td>\n      <td>1.445132</td>\n      <td>0.527112</td>\n      <td>0.255640</td>\n      <td>-0.144263</td>\n      <td>0.227086</td>\n      <td>0.275860</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>-0.495823</td>\n      <td>0.285021</td>\n      <td>-0.201532</td>\n      <td>0.203451</td>\n      <td>0.532686</td>\n      <td>-1.204964</td>\n      <td>0.804856</td>\n      <td>1.632865</td>\n      <td>-0.424039</td>\n      <td>-0.310546</td>\n      <td>...</td>\n      <td>0.860558</td>\n      <td>0.284104</td>\n      <td>0.503761</td>\n      <td>0.064216</td>\n      <td>1.566406</td>\n      <td>0.640111</td>\n      <td>0.259832</td>\n      <td>0.176842</td>\n      <td>-0.001190</td>\n      <td>0.195558</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>-0.461833</td>\n      <td>0.290106</td>\n      <td>-0.094030</td>\n      <td>0.175009</td>\n      <td>0.419192</td>\n      <td>-1.051559</td>\n      <td>0.702931</td>\n      <td>1.599988</td>\n      <td>-0.461818</td>\n      <td>-0.354757</td>\n      <td>...</td>\n      <td>0.788346</td>\n      <td>0.299937</td>\n      <td>0.548200</td>\n      <td>0.047196</td>\n      <td>1.474246</td>\n      <td>0.553182</td>\n      <td>0.250260</td>\n      <td>-0.141310</td>\n      <td>0.191782</td>\n      <td>0.260403</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 100 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"y_pred = classifier.predict(X_test)\nprint('Testing accuracy %s' % accuracy_score(y_test, y_pred))\nprint('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:22.669897Z","iopub.execute_input":"2023-03-17T15:47:22.670340Z","iopub.status.idle":"2023-03-17T15:47:22.707853Z","shell.execute_reply.started":"2023-03-17T15:47:22.670298Z","shell.execute_reply":"2023-03-17T15:47:22.706410Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Testing accuracy 0.319\nTesting F1 score: 0.26810661802268365\n","output_type":"stream"}]},{"cell_type":"code","source":"sent=['i am sad ']\nsent=pd.DataFrame(data=sent,columns=['data'])\nsent=sent['data'].apply(gensim.utils.simple_preprocess)\nprint(sent)\ndocuments=[]\nfor x in sent:\n    document = [word for word in x if word in model.wv.index_to_key]\n    documents.append(document)\nprint(documents)\ncounter = 0\nfor i in range (0,len(documents)):\n    if documents[i] == []:\n        counter += 1\ndocuments=[' '.join(i) for i in documents]\ndocuments=pd.Series(data=documents)\nsent=documents.apply(lambda x:sentence_to_vector(model,x))\nsent=pd.DataFrame(item for item in sent)\nsent","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:22.710059Z","iopub.execute_input":"2023-03-17T15:47:22.710440Z","iopub.status.idle":"2023-03-17T15:47:22.749683Z","shell.execute_reply.started":"2023-03-17T15:47:22.710406Z","shell.execute_reply":"2023-03-17T15:47:22.748404Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"0    [am, sad]\nName: data, dtype: object\n[['am', 'sad']]\n","output_type":"stream"},{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"         0         1         2         3         4         5         6   \\\n0 -0.212139  0.265411 -0.158418  0.043399  0.498835 -1.125694  0.433488   \n\n         7         8         9   ...       90        91        92       93  \\\n0  1.419979 -0.676952 -0.468987  ...  0.76359  0.527688  0.378667  0.11273   \n\n         94      95        96        97        98        99  \n0  0.941798  0.4444  0.190933 -0.409815 -0.192134  0.421695  \n\n[1 rows x 100 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n      <th>95</th>\n      <th>96</th>\n      <th>97</th>\n      <th>98</th>\n      <th>99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.212139</td>\n      <td>0.265411</td>\n      <td>-0.158418</td>\n      <td>0.043399</td>\n      <td>0.498835</td>\n      <td>-1.125694</td>\n      <td>0.433488</td>\n      <td>1.419979</td>\n      <td>-0.676952</td>\n      <td>-0.468987</td>\n      <td>...</td>\n      <td>0.76359</td>\n      <td>0.527688</td>\n      <td>0.378667</td>\n      <td>0.11273</td>\n      <td>0.941798</td>\n      <td>0.4444</td>\n      <td>0.190933</td>\n      <td>-0.409815</td>\n      <td>-0.192134</td>\n      <td>0.421695</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 100 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"emo_la[int(classifier.predict(sent))]","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:22.751545Z","iopub.execute_input":"2023-03-17T15:47:22.752031Z","iopub.status.idle":"2023-03-17T15:47:22.767987Z","shell.execute_reply.started":"2023-03-17T15:47:22.751984Z","shell.execute_reply":"2023-03-17T15:47:22.766808Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"'anger'"},"metadata":{}}]},{"cell_type":"markdown","source":"#### new_word2vec","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef remove_tags(raw_text):\n    cleaned_text = re.sub(re.compile('<.*?>'), '', raw_text)\n    return cleaned_text","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:22.769604Z","iopub.execute_input":"2023-03-17T15:47:22.769950Z","iopub.status.idle":"2023-03-17T15:47:22.779841Z","shell.execute_reply.started":"2023-03-17T15:47:22.769914Z","shell.execute_reply":"2023-03-17T15:47:22.778602Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"df=data.copy()\ndf.drop_duplicates(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:22.781330Z","iopub.execute_input":"2023-03-17T15:47:22.781771Z","iopub.status.idle":"2023-03-17T15:47:22.801502Z","shell.execute_reply.started":"2023-03-17T15:47:22.781736Z","shell.execute_reply":"2023-03-17T15:47:22.800543Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x:x.lower())","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:22.802522Z","iopub.execute_input":"2023-03-17T15:47:22.802894Z","iopub.status.idle":"2023-03-17T15:47:22.817243Z","shell.execute_reply.started":"2023-03-17T15:47:22.802843Z","shell.execute_reply":"2023-03-17T15:47:22.815655Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\nsw_list = stopwords.words('english')\n\ndf['text'] = df['text'].apply(lambda x: [item for item in x.split() if item not in sw_list]).apply(lambda x:\" \".join(x))","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:22.819062Z","iopub.execute_input":"2023-03-17T15:47:22.819418Z","iopub.status.idle":"2023-03-17T15:47:23.511253Z","shell.execute_reply.started":"2023-03-17T15:47:22.819387Z","shell.execute_reply":"2023-03-17T15:47:23.509722Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"import gensim\nfrom nltk import sent_tokenize\nfrom gensim.utils import simple_preprocess\nstory = []\nfor doc in df['text']:\n    raw_sent = sent_tokenize(doc)\n    for sent in raw_sent:\n        story.append(simple_preprocess(sent))\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:23.512984Z","iopub.execute_input":"2023-03-17T15:47:23.513431Z","iopub.status.idle":"2023-03-17T15:47:24.179682Z","shell.execute_reply.started":"2023-03-17T15:47:23.513394Z","shell.execute_reply":"2023-03-17T15:47:24.178392Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"model = gensim.models.Word2Vec(\n    window=10,\n    min_count=1\n)\nmodel.build_vocab(story)\nmodel.train(story, total_examples=model.corpus_count, epochs=model.epochs)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:24.181441Z","iopub.execute_input":"2023-03-17T15:47:24.182101Z","iopub.status.idle":"2023-03-17T15:47:25.477152Z","shell.execute_reply.started":"2023-03-17T15:47:24.182066Z","shell.execute_reply":"2023-03-17T15:47:25.475945Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"(640263, 746965)"},"metadata":{}}]},{"cell_type":"code","source":"def document_vector(doc):\n    # remove out-of-vocabulary words\n    doc = [word for word in doc.split() if word in model.wv.index_to_key]\n    return np.mean(model.wv[doc], axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:25.478729Z","iopub.execute_input":"2023-03-17T15:47:25.479442Z","iopub.status.idle":"2023-03-17T15:47:25.485961Z","shell.execute_reply.started":"2023-03-17T15:47:25.479394Z","shell.execute_reply":"2023-03-17T15:47:25.484708Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"df.loc[8823]","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:25.487357Z","iopub.execute_input":"2023-03-17T15:47:25.487846Z","iopub.status.idle":"2023-03-17T15:47:25.503176Z","shell.execute_reply.started":"2023-03-17T15:47:25.487802Z","shell.execute_reply":"2023-03-17T15:47:25.501703Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"text     feel kind petty blogging\nlabel                           3\nName: 8823, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm import tqdm\nX = []\nfor doc in tqdm(df['text'].values):\n    X.append(document_vector(doc))","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:25.504827Z","iopub.execute_input":"2023-03-17T15:47:25.505232Z","iopub.status.idle":"2023-03-17T15:47:31.604587Z","shell.execute_reply.started":"2023-03-17T15:47:25.505196Z","shell.execute_reply":"2023-03-17T15:47:31.603222Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stderr","text":"100%|██████████| 15999/15999 [00:06<00:00, 2627.81it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"X = np.array(X)\ny=df['label']","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:31.605898Z","iopub.execute_input":"2023-03-17T15:47:31.606255Z","iopub.status.idle":"2023-03-17T15:47:31.630377Z","shell.execute_reply.started":"2023-03-17T15:47:31.606221Z","shell.execute_reply":"2023-03-17T15:47:31.628660Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nrf = RandomForestClassifier()\nrf.fit(X_train,y_train)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:31.631949Z","iopub.execute_input":"2023-03-17T15:47:31.632317Z","iopub.status.idle":"2023-03-17T15:47:51.120775Z","shell.execute_reply.started":"2023-03-17T15:47:31.632286Z","shell.execute_reply":"2023-03-17T15:47:51.119587Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"RandomForestClassifier()"},"metadata":{}}]},{"cell_type":"code","source":"y_pred = rf.predict(X_test)\naccuracy_score(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:47:51.122205Z","iopub.execute_input":"2023-03-17T15:47:51.122645Z","iopub.status.idle":"2023-03-17T15:47:51.250216Z","shell.execute_reply.started":"2023-03-17T15:47:51.122613Z","shell.execute_reply":"2023-03-17T15:47:51.249053Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"0.33875"},"metadata":{}}]},{"cell_type":"markdown","source":"Bert Model\n","metadata":{}},{"cell_type":"code","source":"pip install transformers ","metadata":{"execution":{"iopub.status.busy":"2023-03-18T06:04:21.209155Z","iopub.execute_input":"2023-03-18T06:04:21.209544Z","iopub.status.idle":"2023-03-18T06:04:32.279990Z","shell.execute_reply.started":"2023-03-18T06:04:21.209515Z","shell.execute_reply":"2023-03-18T06:04:32.278735Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.12.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.11)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertTokenizer, TFBertForSequenceClassification\nfrom transformers import InputExample, InputFeatures\n\nmodel = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=6)\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-03-18T06:01:55.417338Z","iopub.execute_input":"2023-03-18T06:01:55.417763Z","iopub.status.idle":"2023-03-18T06:02:19.001967Z","shell.execute_reply.started":"2023-03-18T06:01:55.417725Z","shell.execute_reply":"2023-03-18T06:02:19.000252Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"339a081665454b1badfbcd9cbe14b21e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe045977c0ca4b9a946696debe005a94"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n\nSome layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c444d5a8f72416e8bce3d991d19e351"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a788766f227f4deda9aedec8df69b2d7"}},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX=data_cl['text']\ny=data_cl['label']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2","metadata":{"execution":{"iopub.status.busy":"2023-03-18T07:43:02.467349Z","iopub.execute_input":"2023-03-18T07:43:02.467720Z","iopub.status.idle":"2023-03-18T07:43:02.479502Z","shell.execute_reply.started":"2023-03-18T07:43:02.467693Z","shell.execute_reply":"2023-03-18T07:43:02.478216Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"train=pd.DataFrame()\ntest=pd.DataFrame()\ntrain['text']=X_train\ntrain['sentiment']=y_train\ntest['text']=X_test\ntest['sentiment']=y_test","metadata":{"execution":{"iopub.status.busy":"2023-03-18T07:43:05.742957Z","iopub.execute_input":"2023-03-18T07:43:05.743333Z","iopub.status.idle":"2023-03-18T07:43:05.754632Z","shell.execute_reply.started":"2023-03-18T07:43:05.743303Z","shell.execute_reply":"2023-03-18T07:43:05.753554Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def convert_data_to_examples(train, test, text, sentiment): \n    train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n                                                          text_a = x[text], \n                                                          label = int(x[sentiment])), axis = 1)\n\n    validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n                                                          text_a = x[text], \n                                                          label = int(x[sentiment])), axis = 1,)\n  \n    return train_InputExamples, validation_InputExamples\n\ntrain_InputExamples, validation_InputExamples = convert_data_to_examples(train,  test, 'text',  'sentiment')","metadata":{"execution":{"iopub.status.busy":"2023-03-18T06:03:36.347570Z","iopub.execute_input":"2023-03-18T06:03:36.349369Z","iopub.status.idle":"2023-03-18T06:03:36.560083Z","shell.execute_reply.started":"2023-03-18T06:03:36.349330Z","shell.execute_reply":"2023-03-18T06:03:36.559212Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ndef convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n    features = [] # -> will hold InputFeatures to be converted later\n\n    for e in tqdm(examples):\n        input_dict = tokenizer.encode_plus(\n            e.text_a,\n            add_special_tokens=True,    # Add 'CLS' and 'SEP'\n            max_length=max_length,    # truncates if len(s) > max_length\n            return_token_type_ids=True,\n            return_attention_mask=True,\n            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n            truncation=True\n        )\n\n        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n        features.append(InputFeatures( input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label) )\n\n    def gen():\n        for f in features:\n            yield (\n                {\n                    \"input_ids\": f.input_ids,\n                    \"attention_mask\": f.attention_mask,\n                    \"token_type_ids\": f.token_type_ids,\n                },\n                f.label,\n            )\n\n    return tf.data.Dataset.from_generator(\n        gen,\n        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n        (\n            {\n                \"input_ids\": tf.TensorShape([None]),\n                \"attention_mask\": tf.TensorShape([None]),\n                \"token_type_ids\": tf.TensorShape([None]),\n            },\n            tf.TensorShape([]),\n        ),\n    )\n\n\nDATA_COLUMN = 'text'\nLABEL_COLUMN = 'sentiment'","metadata":{"execution":{"iopub.status.busy":"2023-03-18T06:05:52.220347Z","iopub.execute_input":"2023-03-18T06:05:52.220712Z","iopub.status.idle":"2023-03-18T06:05:52.233447Z","shell.execute_reply.started":"2023-03-18T06:05:52.220684Z","shell.execute_reply":"2023-03-18T06:05:52.232148Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntrain_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\ntrain_data = train_data.shuffle(100).batch(32).repeat(2)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T06:05:58.835898Z","iopub.execute_input":"2023-03-18T06:05:58.836268Z","iopub.status.idle":"2023-03-18T06:05:59.789981Z","shell.execute_reply.started":"2023-03-18T06:05:58.836239Z","shell.execute_reply":"2023-03-18T06:05:59.788759Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"  0%|          | 0/9600 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n100%|██████████| 9600/9600 [00:00<00:00, 10900.04it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\nvalidation_data = validation_data.batch(32)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T06:05:59.792971Z","iopub.execute_input":"2023-03-18T06:05:59.793412Z","iopub.status.idle":"2023-03-18T06:06:00.106017Z","shell.execute_reply.started":"2023-03-18T06:05:59.793375Z","shell.execute_reply":"2023-03-18T06:06:00.105037Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"100%|██████████| 3200/3200 [00:00<00:00, 11191.03it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"num_labels = 6\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n\n\nmodel.fit(train_data, epochs=2,steps_per_epoch=115, validation_data=validation_data)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T06:06:00.107181Z","iopub.execute_input":"2023-03-18T06:06:00.107469Z","iopub.status.idle":"2023-03-18T07:26:24.862458Z","shell.execute_reply.started":"2023-03-18T06:06:00.107445Z","shell.execute_reply":"2023-03-18T07:26:24.861752Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Epoch 1/2\n115/115 [==============================] - 2422s 21s/step - loss: 1.3716 - accuracy: 0.4655 - val_loss: 0.8368 - val_accuracy: 0.7131\nEpoch 2/2\n115/115 [==============================] - 2402s 21s/step - loss: 0.6028 - accuracy: 0.7970 - val_loss: 0.5058 - val_accuracy: 0.8281\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7fe098bc8bd0>"},"metadata":{}}]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2023-03-18T07:26:24.864986Z","iopub.execute_input":"2023-03-18T07:26:24.865802Z","iopub.status.idle":"2023-03-18T07:26:24.871693Z","shell.execute_reply.started":"2023-03-18T07:26:24.865777Z","shell.execute_reply":"2023-03-18T07:26:24.871037Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"<RepeatDataset shapes: ({input_ids: (None, None), attention_mask: (None, None), token_type_ids: (None, None)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_sentences = [' dont know who lnocked the door ' , 'Wow, blew my mind, what a movie by Marvel, animation and story is amazing']\ntf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')   # we are tokenizing before sending into our trained model\ntf_outputs = model(tf_batch)                                  \ntf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)       # axis=-1, this means that the index that will be returned by argmax will be taken from the *last* axis.\n# labels = ['Negative','Positive']\nlabel = tf.argmax(tf_predictions, axis=1)\nlabel = label.numpy()\nfor i in range(len(pred_sentences)):\n    print(pred_sentences[i], \": \", emo_la[label[i]])","metadata":{"execution":{"iopub.status.busy":"2023-03-18T09:23:47.455175Z","iopub.execute_input":"2023-03-18T09:23:47.455570Z","iopub.status.idle":"2023-03-18T09:23:47.696423Z","shell.execute_reply.started":"2023-03-18T09:23:47.455541Z","shell.execute_reply":"2023-03-18T09:23:47.694810Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":" dont know who lnocked the door  :  fear\nWow, blew my mind, what a movie by Marvel, animation and story is amazing :  surprise\n","output_type":"stream"}]},{"cell_type":"code","source":"tf_predictions","metadata":{"execution":{"iopub.status.busy":"2023-03-18T07:26:25.084515Z","iopub.execute_input":"2023-03-18T07:26:25.084775Z","iopub.status.idle":"2023-03-18T07:26:25.092709Z","shell.execute_reply.started":"2023-03-18T07:26:25.084751Z","shell.execute_reply":"2023-03-18T07:26:25.091310Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(2, 6), dtype=float32, numpy=\narray([[0.38976142, 0.07313882, 0.0518993 , 0.13277097, 0.32578304,\n        0.02664651],\n       [0.01684946, 0.12734203, 0.02270525, 0.01233886, 0.06884786,\n        0.7519165 ]], dtype=float32)>"},"metadata":{}}]},{"cell_type":"code","source":"X_val\ntest_set=[i for i in X_val]","metadata":{"execution":{"iopub.status.busy":"2023-03-18T07:43:12.950599Z","iopub.execute_input":"2023-03-18T07:43:12.950960Z","iopub.status.idle":"2023-03-18T07:43:12.960912Z","shell.execute_reply.started":"2023-03-18T07:43:12.950933Z","shell.execute_reply":"2023-03-18T07:43:12.959529Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"def predict_bert(data):\n    tf_batch = tokenizer(data, max_length=128, padding=True, truncation=True, return_tensors='tf')   # we are tokenizing before sending into our trained model\n    tf_outputs = model(tf_batch)                                  \n    tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)       # axis=-1, this means that the index that will be returned by argmax will be taken from the *last* axis.\n    # labels = ['Negative','Positive']\n    label = tf.argmax(tf_predictions, axis=1)\n    y_pred = label.numpy()\n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2023-03-18T07:29:55.257237Z","iopub.execute_input":"2023-03-18T07:29:55.257618Z","iopub.status.idle":"2023-03-18T07:29:55.263899Z","shell.execute_reply.started":"2023-03-18T07:29:55.257590Z","shell.execute_reply":"2023-03-18T07:29:55.262776Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.metrics import confusion_matrix\ny_pred=predict_bert(test_set)\nconfusion_matrix(y_val, y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T07:43:16.943381Z","iopub.execute_input":"2023-03-18T07:43:16.943743Z","iopub.status.idle":"2023-03-18T07:46:13.941867Z","shell.execute_reply.started":"2023-03-18T07:43:16.943714Z","shell.execute_reply":"2023-03-18T07:46:13.940799Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"array([[887,  11,  12,  10,  36,   3],\n       [  9, 941,  75,   2,   1,  16],\n       [  8,  20, 217,   1,   2,   1],\n       [ 71,   4,   2, 311,  47,   5],\n       [ 10,   3,   1,   2, 376,  14],\n       [  1,   0,   1,   1,  17,  82]])"},"metadata":{}}]},{"cell_type":"code","source":"confusion_matrix(y_val, y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T07:46:21.401454Z","iopub.execute_input":"2023-03-18T07:46:21.401835Z","iopub.status.idle":"2023-03-18T07:46:21.410087Z","shell.execute_reply.started":"2023-03-18T07:46:21.401806Z","shell.execute_reply":"2023-03-18T07:46:21.408582Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"array([[887,  11,  12,  10,  36,   3],\n       [  9, 941,  75,   2,   1,  16],\n       [  8,  20, 217,   1,   2,   1],\n       [ 71,   4,   2, 311,  47,   5],\n       [ 10,   3,   1,   2, 376,  14],\n       [  1,   0,   1,   1,  17,  82]])"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nprint(classification_report(y_val, y_pred))\nprint(accuracy_score(y_val, y_pred))\nprint(emo_la)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T07:46:24.910677Z","iopub.execute_input":"2023-03-18T07:46:24.911039Z","iopub.status.idle":"2023-03-18T07:46:24.924734Z","shell.execute_reply.started":"2023-03-18T07:46:24.911012Z","shell.execute_reply":"2023-03-18T07:46:24.923546Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.90      0.92      0.91       959\n           1       0.96      0.90      0.93      1044\n           2       0.70      0.87      0.78       249\n           3       0.95      0.71      0.81       440\n           4       0.78      0.93      0.85       406\n           5       0.68      0.80      0.74       102\n\n    accuracy                           0.88      3200\n   macro avg       0.83      0.86      0.84      3200\nweighted avg       0.89      0.88      0.88      3200\n\n0.879375\n{0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save('bert_model')\n","metadata":{"execution":{"iopub.status.busy":"2023-03-18T07:53:54.034967Z","iopub.execute_input":"2023-03-18T07:53:54.035350Z","iopub.status.idle":"2023-03-18T07:54:25.038147Z","shell.execute_reply.started":"2023-03-18T07:53:54.035319Z","shell.execute_reply":"2023-03-18T07:54:25.037291Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"model2 = tf.keras.models.clone_model(model)\n# Verify that the architecture is the same\nassert model1.summary() == model2.summary()\n\n# Verify that the weights are the same\nassert model1.get_weights() == model2.get_weights()","metadata":{"execution":{"iopub.status.busy":"2023-03-18T07:26:25.337517Z","iopub.status.idle":"2023-03-18T07:26:25.337969Z","shell.execute_reply.started":"2023-03-18T07:26:25.337731Z","shell.execute_reply":"2023-03-18T07:26:25.337753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working/bert_model","metadata":{"execution":{"iopub.status.busy":"2023-03-18T07:58:32.356264Z","iopub.execute_input":"2023-03-18T07:58:32.356651Z","iopub.status.idle":"2023-03-18T07:59:36.499721Z","shell.execute_reply.started":"2023-03-18T07:58:32.356623Z","shell.execute_reply":"2023-03-18T07:59:36.498526Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"  adding: kaggle/working/bert_model/ (stored 0%)\n  adding: kaggle/working/bert_model/saved_model.pb (deflated 92%)\n  adding: kaggle/working/bert_model/variables/ (stored 0%)\n  adding: kaggle/working/bert_model/variables/variables.index (deflated 80%)\n  adding: kaggle/working/bert_model/variables/variables.data-00000-of-00001 (deflated 18%)\n  adding: kaggle/working/bert_model/keras_metadata.pb (deflated 95%)\n  adding: kaggle/working/bert_model/assets/ (stored 0%)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Simple bert","metadata":{}},{"cell_type":"code","source":"# from transformers import BertTokenizer, TFBertForSequenceClassification\n# from transformers import InputExample, InputFeatures\n\n# model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=6)\n# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-03-18T10:29:02.899637Z","iopub.execute_input":"2023-03-18T10:29:02.900163Z","iopub.status.idle":"2023-03-18T10:29:30.592487Z","shell.execute_reply.started":"2023-03-18T10:29:02.900053Z","shell.execute_reply":"2023-03-18T10:29:30.591151Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7ce23b4552f4bd39c47526de107fbd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b56ca6c039345bdac1efa85e22d1cc5"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n\nSome layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"072cec00f11548ed80a10ba8b9c5dd73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"730b625b33a548f59964bff5a62f5e14"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-03-18T11:14:46.329477Z","iopub.execute_input":"2023-03-18T11:14:46.329929Z","iopub.status.idle":"2023-03-18T11:15:07.217713Z","shell.execute_reply.started":"2023-03-18T11:14:46.329830Z","shell.execute_reply":"2023-03-18T11:15:07.216844Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:58:49.484867Z","iopub.execute_input":"2023-03-18T15:58:49.485396Z","iopub.status.idle":"2023-03-18T15:58:59.233486Z","shell.execute_reply.started":"2023-03-18T15:58:49.485357Z","shell.execute_reply":"2023-03-18T15:58:59.232227Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\nmodel = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\",num_labels=6)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:58:59.235437Z","iopub.execute_input":"2023-03-18T15:58:59.236132Z","iopub.status.idle":"2023-03-18T15:59:13.155582Z","shell.execute_reply.started":"2023-03-18T15:58:59.236095Z","shell.execute_reply":"2023-03-18T15:59:13.154413Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"946a3f1d5fcf4bb3a58b5bda424c2df0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e027f07f5924f55a73cd9d079ba4348"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bccbf5b69564859a4f498a9b6bfadb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/347M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62e5440adb544340a77034e6ff4cf6aa"}},"metadata":{}},{"name":"stderr","text":"Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_transform', 'vocab_projector', 'activation_13']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_19']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX=data_cl['text']\ny=data_cl['label']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:59:13.157022Z","iopub.execute_input":"2023-03-18T15:59:13.157378Z","iopub.status.idle":"2023-03-18T15:59:13.174495Z","shell.execute_reply.started":"2023-03-18T15:59:13.157346Z","shell.execute_reply":"2023-03-18T15:59:13.173104Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train=pd.DataFrame()\ntest=pd.DataFrame()\ntrain['text']=X_train\ntrain['sentiment']=y_train\ntest['text']=X_test\ntest['sentiment']=y_test","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:59:13.178454Z","iopub.execute_input":"2023-03-18T15:59:13.179042Z","iopub.status.idle":"2023-03-18T15:59:13.193035Z","shell.execute_reply.started":"2023-03-18T15:59:13.178993Z","shell.execute_reply":"2023-03-18T15:59:13.191805Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from transformers import InputExample, InputFeatures","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:59:13.195151Z","iopub.execute_input":"2023-03-18T15:59:13.195814Z","iopub.status.idle":"2023-03-18T15:59:13.202975Z","shell.execute_reply.started":"2023-03-18T15:59:13.195765Z","shell.execute_reply":"2023-03-18T15:59:13.201665Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def convert_data_to_examples(train, test, text, sentiment): \n    train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n                                                          text_a = x[text], \n                                                          label = int(x[sentiment])), axis = 1)\n\n    validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n                                                          text_a = x[text], \n                                                          label = int(x[sentiment])), axis = 1,)\n  \n    return train_InputExamples, validation_InputExamples\n\ntrain_InputExamples, validation_InputExamples = convert_data_to_examples(train,  test, 'text',  'sentiment')","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:59:13.204573Z","iopub.execute_input":"2023-03-18T15:59:13.205128Z","iopub.status.idle":"2023-03-18T15:59:13.444058Z","shell.execute_reply.started":"2023-03-18T15:59:13.205077Z","shell.execute_reply":"2023-03-18T15:59:13.443004Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ndef convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n    features = [] # -> will hold InputFeatures to be converted later\n\n    for e in tqdm(examples):\n        input_dict = tokenizer.encode_plus(\n            e.text_a,\n            add_special_tokens=True,    # Add 'CLS' and 'SEP'\n            max_length=max_length,    # truncates if len(s) > max_length\n#             return_token_type_ids=True,\n            return_attention_mask=True,\n            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n            truncation=True\n        )\n\n        input_ids, attention_mask = (input_dict[\"input_ids\"], input_dict['attention_mask'])\n        features.append(InputFeatures( input_ids=input_ids, attention_mask=attention_mask,  label=e.label) )\n\n    def gen():\n        for f in features:\n            yield (\n                {\n                    \"input_ids\": f.input_ids,\n                    \"attention_mask\": f.attention_mask,\n#                     \"token_type_ids\": f.token_type_ids,\n                },\n                f.label,\n            )\n\n    return tf.data.Dataset.from_generator(\n        gen,\n        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32}, tf.int64),\n        (\n            {\n                \"input_ids\": tf.TensorShape([None]),\n                \"attention_mask\": tf.TensorShape([None]),\n#                 \"token_type_ids\": tf.TensorShape([None]),\n            },\n            tf.TensorShape([]),\n        ),\n    )\n\n\nDATA_COLUMN = 'text'\nLABEL_COLUMN = 'sentiment'","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:59:13.445676Z","iopub.execute_input":"2023-03-18T15:59:13.446834Z","iopub.status.idle":"2023-03-18T15:59:13.459409Z","shell.execute_reply.started":"2023-03-18T15:59:13.446781Z","shell.execute_reply":"2023-03-18T15:59:13.458021Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntrain_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\ntrain_data = train_data.shuffle(100).batch(32).repeat(2)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:59:13.461143Z","iopub.execute_input":"2023-03-18T15:59:13.461514Z","iopub.status.idle":"2023-03-18T15:59:18.741769Z","shell.execute_reply.started":"2023-03-18T15:59:13.461480Z","shell.execute_reply":"2023-03-18T15:59:18.740533Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"  0%|          | 0/9600 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n100%|██████████| 9600/9600 [00:05<00:00, 1851.07it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\nvalidation_data = validation_data.batch(32)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:59:18.746013Z","iopub.execute_input":"2023-03-18T15:59:18.746394Z","iopub.status.idle":"2023-03-18T15:59:20.490144Z","shell.execute_reply.started":"2023-03-18T15:59:18.746362Z","shell.execute_reply":"2023-03-18T15:59:20.488874Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"100%|██████████| 3200/3200 [00:01<00:00, 1871.78it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install tensorflow-model-optimization","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:59:20.493444Z","iopub.execute_input":"2023-03-18T15:59:20.493978Z","iopub.status.idle":"2023-03-18T15:59:34.019278Z","shell.execute_reply.started":"2023-03-18T15:59:20.493940Z","shell.execute_reply":"2023-03-18T15:59:34.017682Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Collecting tensorflow-model-optimization\n  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy~=1.14 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-optimization) (1.21.6)\nRequirement already satisfied: dm-tree~=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-optimization) (0.1.7)\nRequirement already satisfied: six~=1.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-optimization) (1.15.0)\nInstalling collected packages: tensorflow-model-optimization\nSuccessfully installed tensorflow-model-optimization-0.7.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow_model_optimization \nfrom tensorflow_model_optimization.sparsity import keras as sparsity\n# Define the pruning schedule\npruning_schedule = sparsity.PolynomialDecay(initial_sparsity=0.50,\n                                             final_sparsity=0.90,\n                                             begin_step=2000,\n                                             end_step=4000)\n\n# Create a pruning callback that will apply pruning to the model\npruning_callbacks = [sparsity.UpdatePruningStep()]","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:59:34.021663Z","iopub.execute_input":"2023-03-18T15:59:34.022446Z","iopub.status.idle":"2023-03-18T15:59:34.219940Z","shell.execute_reply.started":"2023-03-18T15:59:34.022403Z","shell.execute_reply":"2023-03-18T15:59:34.218620Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n\n\nmodel.fit(train_data, epochs=2,steps_per_epoch=115, validation_data=validation_data,callbacks=pruning_callbacks)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T15:59:34.222167Z","iopub.execute_input":"2023-03-18T15:59:34.222694Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/2\n115/115 [==============================] - 1746s 15s/step - loss: 1.1352 - accuracy: 0.5962 - val_loss: 0.6264 - val_accuracy: 0.7875\nEpoch 2/2\n 45/115 [==========>...................] - ETA: 13:03 - loss: 0.5050 - accuracy: 0.8417","output_type":"stream"}]},{"cell_type":"code","source":"pred_sentences = [' interview tomorrow,dont know if iam prepared ' , 'Wow, blew my mind, what a movie by Marvel, animation and story is amazing']\ntf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')   # we are tokenizing before sending into our trained model\ntf_outputs = model(tf_batch)                                  \ntf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)       # axis=-1, this means that the index that will be returned by argmax will be taken from the *last* axis.\n# labels = ['Negative','Positive']\nlabel = tf.argmax(tf_predictions, axis=1)\nlabel = label.numpy()\nfor i in range(len(pred_sentences)):\n    print(pred_sentences[i], \": \", emo_la[label[i]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_bert(data):\n    tf_batch = tokenizer(data, max_length=128, padding=True, truncation=True, return_tensors='tf')   # we are tokenizing before sending into our trained model\n    tf_outputs = model(tf_batch)                                  \n    tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)       # axis=-1, this means that the index that will be returned by argmax will be taken from the *last* axis.\n    # labels = ['Negative','Positive']\n    label = tf.argmax(tf_predictions, axis=1)\n    y_pred = label.numpy()\n    return y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val\ntest_set=[i for i in X_val]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ny_pred=predict_bert(test_set)\nconfusion_matrix(y_val, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nprint(classification_report(y_val, y_pred))\nprint(accuracy_score(y_val, y_pred))\nprint(emo_la)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('bert_model')\n!zip -r file.zip /kaggle/working/bert_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'file.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}